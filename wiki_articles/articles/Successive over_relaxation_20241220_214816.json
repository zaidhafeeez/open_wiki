{
  "title": "Successive over-relaxation",
  "summary": "In numerical linear algebra, the method of successive over-relaxation (SOR) is a variant of the Gauss–Seidel method for solving a linear system of equations, resulting in faster convergence. A similar method can be used for any slowly converging iterative process.\nIt was devised simultaneously by David M. Young Jr. and by Stanley P. Frankel in 1950 for the purpose of automatically solving linear systems on digital computers. Over-relaxation methods had been used before the work of Young and Fran",
  "content": "---\ntitle: Successive over-relaxation\nurl: https://en.wikipedia.org/wiki/Successive_over-relaxation\nlanguage: en\ncategories: [\"Category:Articles with example Python (programming language) code\", \"Category:Articles with example pseudocode\", \"Category:Articles with short description\", \"Category:Numerical linear algebra\", \"Category:Relaxation (iterative methods)\", \"Category:Short description matches Wikidata\", \"Category:Wikipedia articles licensed under the GNU Free Document License\"]\nreferences: 0\nlast_modified: 2024-12-19T13:53:56Z\n---\n\n# Successive over-relaxation\n\n## Summary\n\nIn numerical linear algebra, the method of successive over-relaxation (SOR) is a variant of the Gauss–Seidel method for solving a linear system of equations, resulting in faster convergence. A similar method can be used for any slowly converging iterative process.\nIt was devised simultaneously by David M. Young Jr. and by Stanley P. Frankel in 1950 for the purpose of automatically solving linear systems on digital computers. Over-relaxation methods had been used before the work of Young and Fran\n\n## Full Content\n\nIn numerical linear algebra, the method of successive over-relaxation (SOR) is a variant of the Gauss–Seidel method for solving a linear system of equations, resulting in faster convergence. A similar method can be used for any slowly converging iterative process.\nIt was devised simultaneously by David M. Young Jr. and by Stanley P. Frankel in 1950 for the purpose of automatically solving linear systems on digital computers. Over-relaxation methods had been used before the work of Young and Frankel. An example is the method of Lewis Fry Richardson, and the methods developed by R. V. Southwell. However, these methods were designed for computation by human calculators, requiring some expertise to ensure convergence to the solution which made them inapplicable for programming on digital computers. These aspects are discussed in the thesis of David M. Young Jr.\n\nFormulation\nGiven a square system of n linear equations with unknown x:\n\n  \n    \n      \n        A\n        \n          x\n        \n        =\n        \n          b\n        \n      \n    \n    {\\displaystyle A\\mathbf {x} =\\mathbf {b} }\n  \n\nwhere:\n\n  \n    \n      \n        A\n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    a\n                    \n                      11\n                    \n                  \n                \n                \n                  \n                    a\n                    \n                      12\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    a\n                    \n                      1\n                      n\n                    \n                  \n                \n              \n              \n                \n                  \n                    a\n                    \n                      21\n                    \n                  \n                \n                \n                  \n                    a\n                    \n                      22\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    a\n                    \n                      2\n                      n\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    a\n                    \n                      n\n                      1\n                    \n                  \n                \n                \n                  \n                    a\n                    \n                      n\n                      2\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    a\n                    \n                      n\n                      n\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        ,\n        \n        \n          x\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    x\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    x\n                    \n                      n\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        ,\n        \n        \n          b\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    b\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    b\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    b\n                    \n                      n\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle A={\\begin{bmatrix}a_{11}&a_{12}&\\cdots &a_{1n}\\\\a_{21}&a_{22}&\\cdots &a_{2n}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\a_{n1}&a_{n2}&\\cdots &a_{nn}\\end{bmatrix}},\\qquad \\mathbf {x} ={\\begin{bmatrix}x_{1}\\\\x_{2}\\\\\\vdots \\\\x_{n}\\end{bmatrix}},\\qquad \\mathbf {b} ={\\begin{bmatrix}b_{1}\\\\b_{2}\\\\\\vdots \\\\b_{n}\\end{bmatrix}}.}\n  \n\nThen A can be decomposed into a diagonal component D, and strictly lower and upper triangular components L and U:\n\n  \n    \n      \n        A\n        =\n        D\n        +\n        L\n        +\n        U\n        ,\n      \n    \n    {\\displaystyle A=D+L+U,}\n  \n\nwhere\n\n  \n    \n      \n        D\n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    a\n                    \n                      11\n                    \n                  \n                \n                \n                  0\n                \n                \n                  ⋯\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  \n                    a\n                    \n                      22\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  0\n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n                \n                  ⋮\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  ⋯\n                \n                \n                  \n                    a\n                    \n                      n\n                      n\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        ,\n        \n        L\n        =\n        \n          \n            [\n            \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  ⋯\n                \n                \n                  0\n                \n              \n              \n                \n                  \n                    a\n                    \n                      21\n                    \n                  \n                \n                \n                  0\n                \n                \n                  ⋯\n                \n                \n                  0\n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    a\n                    \n                      n\n                      1\n                    \n                  \n                \n                \n                  \n                    a\n                    \n                      n\n                      2\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  0\n                \n              \n            \n            ]\n          \n        \n        ,\n        \n        U\n        =\n        \n          \n            [\n            \n              \n                \n                  0\n                \n                \n                  \n                    a\n                    \n                      12\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    a\n                    \n                      1\n                      n\n                    \n                  \n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  ⋯\n                \n                \n                  \n                    a\n                    \n                      2\n                      n\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n                \n                  ⋮\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  ⋯\n                \n                \n                  0\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle D={\\begin{bmatrix}a_{11}&0&\\cdots &0\\\\0&a_{22}&\\cdots &0\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\0&0&\\cdots &a_{nn}\\end{bmatrix}},\\quad L={\\begin{bmatrix}0&0&\\cdots &0\\\\a_{21}&0&\\cdots &0\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\a_{n1}&a_{n2}&\\cdots &0\\end{bmatrix}},\\quad U={\\begin{bmatrix}0&a_{12}&\\cdots &a_{1n}\\\\0&0&\\cdots &a_{2n}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\0&0&\\cdots &0\\end{bmatrix}}.}\n  \n\nThe system of linear equations may be rewritten as:\n\n  \n    \n      \n        (\n        D\n        +\n        ω\n        L\n        )\n        \n          x\n        \n        =\n        ω\n        \n          b\n        \n        −\n        [\n        ω\n        U\n        +\n        (\n        ω\n        −\n        1\n        )\n        D\n        ]\n        \n          x\n        \n      \n    \n    {\\displaystyle (D+\\omega L)\\mathbf {x} =\\omega \\mathbf {b} -[\\omega U+(\\omega -1)D]\\mathbf {x} }\n  \n\nfor a constant ω > 1, called the relaxation factor.\nThe method of successive over-relaxation is an iterative technique that solves the left hand side of this expression for x, using the previous value for x on the right hand side. Analytically, this may be written as:\n\n  \n    \n      \n        \n          \n            x\n          \n          \n            (\n            k\n            +\n            1\n            )\n          \n        \n        =\n        (\n        D\n        +\n        ω\n        L\n        \n          )\n          \n            −\n            1\n          \n        \n        \n          \n            (\n          \n        \n        ω\n        \n          b\n        \n        −\n        [\n        ω\n        U\n        +\n        (\n        ω\n        −\n        1\n        )\n        D\n        ]\n        \n          \n            x\n          \n          \n            (\n            k\n            )\n          \n        \n        \n          \n            )\n          \n        \n        =\n        \n          L\n          \n            ω\n          \n        \n        \n          \n            x\n          \n          \n            (\n            k\n            )\n          \n        \n        +\n        \n          c\n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {x} ^{(k+1)}=(D+\\omega L)^{-1}{\\big (}\\omega \\mathbf {b} -[\\omega U+(\\omega -1)D]\\mathbf {x} ^{(k)}{\\big )}=L_{\\omega }\\mathbf {x} ^{(k)}+\\mathbf {c} ,}\n  \n\nwhere \n  \n    \n      \n        \n          \n            x\n          \n          \n            (\n            k\n            )\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} ^{(k)}}\n  \n is the kth approximation or iteration of \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n  \n and \n  \n    \n      \n        \n          \n            x\n          \n          \n            (\n            k\n            +\n            1\n            )\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} ^{(k+1)}}\n  \n is the next or k + 1 iteration of \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n  \n.\nHowever, by taking advantage of the triangular form of (D+ωL), the elements of x(k+1) can be computed sequentially using forward substitution:\n\n  \n    \n      \n        \n          x\n          \n            i\n          \n          \n            (\n            k\n            +\n            1\n            )\n          \n        \n        =\n        (\n        1\n        −\n        ω\n        )\n        \n          x\n          \n            i\n          \n          \n            (\n            k\n            )\n          \n        \n        +\n        \n          \n            ω\n            \n              a\n              \n                i\n                i\n              \n            \n          \n        \n        \n          (\n          \n            \n              b\n              \n                i\n              \n            \n            −\n            \n              ∑\n              \n                j\n                <\n                i\n              \n            \n            \n              a\n              \n                i\n                j\n              \n            \n            \n              x\n              \n                j\n              \n              \n                (\n                k\n                +\n                1\n                )\n              \n            \n            −\n            \n              ∑\n              \n                j\n                >\n                i\n              \n            \n            \n              a\n              \n                i\n                j\n              \n            \n            \n              x\n              \n                j\n              \n              \n                (\n                k\n                )\n              \n            \n          \n          )\n        \n        ,\n        \n        i\n        =\n        1\n        ,\n        2\n        ,\n        …\n        ,\n        n\n        .\n      \n    \n    {\\displaystyle x_{i}^{(k+1)}=(1-\\omega )x_{i}^{(k)}+{\\frac {\\omega }{a_{ii}}}\\left(b_{i}-\\sum _{j<i}a_{ij}x_{j}^{(k+1)}-\\sum _{j>i}a_{ij}x_{j}^{(k)}\\right),\\quad i=1,2,\\ldots ,n.}\n  \n\nThis can again be written analytically in matrix-vector form without the need of inverting the matrix \n  \n    \n      \n        (\n        D\n        +\n        ω\n        L\n        )\n      \n    \n    {\\displaystyle (D+\\omega L)}\n  \n:\n\n  \n    \n      \n        \n          \n            x\n          \n          \n            (\n            k\n            +\n            1\n            )\n          \n        \n        =\n        (\n        1\n        −\n        ω\n        )\n        \n          \n            x\n          \n          \n            (\n            k\n            )\n          \n        \n        +\n        ω\n        \n          D\n          \n            −\n            1\n          \n        \n        \n          \n            (\n          \n        \n        \n          b\n        \n        −\n        L\n        \n          \n            x\n          \n          \n            (\n            k\n            +\n            1\n            )\n          \n        \n        −\n        U\n        \n          \n            x\n          \n          \n            (\n            k\n            )\n          \n        \n        \n          \n            )\n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {x} ^{(k+1)}=(1-\\omega )\\mathbf {x} ^{(k)}+\\omega D^{-1}{\\big (}\\mathbf {b} -L\\mathbf {x} ^{(k+1)}-U\\mathbf {x} ^{(k)}{\\big )}.}\n\nConvergence\nThe choice of relaxation factor ω is not necessarily easy, and depends upon the properties of the coefficient matrix.  \nIn 1947, Ostrowski proved that if \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n is symmetric and positive-definite then \n  \n    \n      \n        ρ\n        (\n        \n          L\n          \n            ω\n          \n        \n        )\n        <\n        1\n      \n    \n    {\\displaystyle \\rho (L_{\\omega })<1}\n  \n for \n  \n    \n      \n        0\n        <\n        ω\n        <\n        2\n      \n    \n    {\\displaystyle 0<\\omega <2}\n  \n. \nThus, convergence of the iteration process follows, but we are generally interested in faster convergence rather than just convergence.\n\nConvergence Rate\nThe convergence rate for the SOR method can be analytically derived.\nOne needs to assume the following\n\nthe relaxation parameter is appropriate: \n  \n    \n      \n        ω\n        ∈\n        (\n        0\n        ,\n        2\n        )\n      \n    \n    {\\displaystyle \\omega \\in (0,2)}\n  \n\nJacobi's iteration matrix \n  \n    \n      \n        \n          C\n          \n            Jac\n          \n        \n        :=\n        I\n        −\n        \n          D\n          \n            −\n            1\n          \n        \n        A\n      \n    \n    {\\displaystyle C_{\\text{Jac}}:=I-D^{-1}A}\n  \n has only real eigenvalues\nJacobi's method is convergent: \n  \n    \n      \n        μ\n        :=\n        ρ\n        (\n        \n          C\n          \n            Jac\n          \n        \n        )\n        <\n        1\n      \n    \n    {\\displaystyle \\mu :=\\rho (C_{\\text{Jac}})<1}\n  \n\nthe matrix decomposition \n  \n    \n      \n        A\n        =\n        D\n        +\n        L\n        +\n        U\n      \n    \n    {\\displaystyle A=D+L+U}\n  \n satisfies the property that \n  \n    \n      \n        det\n        ⁡\n        (\n        λ\n        D\n        +\n        z\n        L\n        +\n        \n          \n            \n              1\n              z\n            \n          \n        \n        U\n        )\n        =\n        det\n        ⁡\n        (\n        λ\n        D\n        +\n        L\n        +\n        U\n        )\n      \n    \n    {\\displaystyle \\operatorname {det} (\\lambda D+zL+{\\tfrac {1}{z}}U)=\\operatorname {det} (\\lambda D+L+U)}\n  \n for any \n  \n    \n      \n        z\n        ∈\n        \n          C\n        \n        ∖\n        {\n        0\n        }\n      \n    \n    {\\displaystyle z\\in \\mathbb {C} \\setminus \\{0\\}}\n  \n and \n  \n    \n      \n        λ\n        ∈\n        \n          C\n        \n      \n    \n    {\\displaystyle \\lambda \\in \\mathbb {C} }\n  \n.\nThen the convergence rate can be expressed as\n\n  \n    \n      \n        ρ\n        (\n        \n          C\n          \n            ω\n          \n        \n        )\n        =\n        \n          \n            {\n            \n              \n                \n                  \n                    \n                      1\n                      4\n                    \n                  \n                  \n                    \n                      (\n                      \n                        ω\n                        μ\n                        +\n                        \n                          \n                            \n                              ω\n                              \n                                2\n                              \n                            \n                            \n                              μ\n                              \n                                2\n                              \n                            \n                            −\n                            4\n                            (\n                            ω\n                            −\n                            1\n                            )\n                          \n                        \n                      \n                      )\n                    \n                    \n                      2\n                    \n                  \n                  \n                  ,\n                \n                \n                  0\n                  <\n                  ω\n                  ≤\n                  \n                    ω\n                    \n                      opt\n                    \n                  \n                \n              \n              \n                \n                  ω\n                  −\n                  1\n                  \n                  ,\n                \n                \n                  \n                    ω\n                    \n                      opt\n                    \n                  \n                  <\n                  ω\n                  <\n                  2\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle \\rho (C_{\\omega })={\\begin{cases}{\\frac {1}{4}}\\left(\\omega \\mu +{\\sqrt {\\omega ^{2}\\mu ^{2}-4(\\omega -1)}}\\right)^{2}\\,,&0<\\omega \\leq \\omega _{\\text{opt}}\\\\\\omega -1\\,,&\\omega _{\\text{opt}}<\\omega <2\\end{cases}}}\n  \n\nwhere the optimal relaxation parameter is given by\n\n  \n    \n      \n        \n          ω\n          \n            opt\n          \n        \n        :=\n        1\n        +\n        \n          \n            (\n            \n              \n                μ\n                \n                  1\n                  +\n                  \n                    \n                      1\n                      −\n                      \n                        μ\n                        \n                          2\n                        \n                      \n                    \n                  \n                \n              \n            \n            )\n          \n          \n            2\n          \n        \n        =\n        1\n        +\n        \n          \n            \n              μ\n              \n                2\n              \n            \n            4\n          \n        \n        +\n        O\n        (\n        \n          μ\n          \n            3\n          \n        \n        )\n        \n        .\n      \n    \n    {\\displaystyle \\omega _{\\text{opt}}:=1+\\left({\\frac {\\mu }{1+{\\sqrt {1-\\mu ^{2}}}}}\\right)^{2}=1+{\\frac {\\mu ^{2}}{4}}+O(\\mu ^{3})\\,.}\n  \n\nIn particular, for \n  \n    \n      \n        ω\n        =\n        1\n      \n    \n    {\\displaystyle \\omega =1}\n  \n (Gauss-Seidel) it holds that \n  \n    \n      \n        ρ\n        (\n        \n          C\n          \n            ω\n          \n        \n        )\n        =\n        \n          μ\n          \n            2\n          \n        \n        =\n        ρ\n        (\n        \n          C\n          \n            Jac\n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\rho (C_{\\omega })=\\mu ^{2}=\\rho (C_{\\text{Jac}})^{2}}\n  \n.\nFor the optimal \n  \n    \n      \n        ω\n      \n    \n    {\\displaystyle \\omega }\n  \n we get \n  \n    \n      \n        ρ\n        (\n        \n          C\n          \n            ω\n          \n        \n        )\n        =\n        \n          \n            \n              1\n              −\n              \n                \n                  1\n                  −\n                  \n                    μ\n                    \n                      2\n                    \n                  \n                \n              \n            \n            \n              1\n              +\n              \n                \n                  1\n                  −\n                  \n                    μ\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              μ\n              \n                2\n              \n            \n            4\n          \n        \n        +\n        O\n        (\n        \n          μ\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle \\rho (C_{\\omega })={\\frac {1-{\\sqrt {1-\\mu ^{2}}}}{1+{\\sqrt {1-\\mu ^{2}}}}}={\\frac {\\mu ^{2}}{4}}+O(\\mu ^{3})}\n  \n, which shows SOR is roughly four times more efficient than Gauss–Seidel.  \nThe last assumption is satisfied for tridiagonal matrices since \n  \n    \n      \n        Z\n        (\n        λ\n        D\n        +\n        L\n        +\n        U\n        )\n        \n          Z\n          \n            −\n            1\n          \n        \n        =\n        λ\n        D\n        +\n        z\n        L\n        +\n        \n          \n            \n              1\n              z\n            \n          \n        \n        U\n      \n    \n    {\\displaystyle Z(\\lambda D+L+U)Z^{-1}=\\lambda D+zL+{\\tfrac {1}{z}}U}\n  \n for diagonal \n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n  \n with entries \n  \n    \n      \n        \n          Z\n          \n            i\n            i\n          \n        \n        =\n        \n          z\n          \n            i\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle Z_{ii}=z^{i-1}}\n  \n and \n  \n    \n      \n        det\n        ⁡\n        (\n        λ\n        D\n        +\n        L\n        +\n        U\n        )\n        =\n        det\n        ⁡\n        (\n        Z\n        (\n        λ\n        D\n        +\n        L\n        +\n        U\n        )\n        \n          Z\n          \n            −\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle \\operatorname {det} (\\lambda D+L+U)=\\operatorname {det} (Z(\\lambda D+L+U)Z^{-1})}\n  \n.\n\nAlgorithm\nSince elements can be overwritten as they are computed in this algorithm, only one storage vector is needed, and vector indexing is omitted. The algorithm goes as follows:\n\nInputs: A, b, ω\nOutput: φ\n\nChoose an initial guess φ to the solution\nrepeat until convergence\n    for i from 1 until n do\n        set σ to 0\n        for j from 1 until n do\n            if j ≠ i then\n                set σ to σ + aij φj\n            end if\n        end (j-loop)\n        set φi to (1 − ω)φi + ω(bi − σ) / aii\n    end (i-loop)\n    check if convergence is reached\nend (repeat)\n\nNote\n\n  \n    \n      \n        (\n        1\n        −\n        ω\n        )\n        \n          ϕ\n          \n            i\n          \n        \n        +\n        \n          \n            ω\n            \n              a\n              \n                i\n                i\n              \n            \n          \n        \n        (\n        \n          b\n          \n            i\n          \n        \n        −\n        σ\n        )\n      \n    \n    {\\displaystyle (1-\\omega )\\phi _{i}+{\\frac {\\omega }{a_{ii}}}(b_{i}-\\sigma )}\n  \n can also be written \n  \n    \n      \n        \n          ϕ\n          \n            i\n          \n        \n        +\n        ω\n        \n          (\n          \n            \n              \n                \n                  \n                    b\n                    \n                      i\n                    \n                  \n                  −\n                  σ\n                \n                \n                  a\n                  \n                    i\n                    i\n                  \n                \n              \n            \n            −\n            \n              ϕ\n              \n                i\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\phi _{i}+\\omega \\left({\\frac {b_{i}-\\sigma }{a_{ii}}}-\\phi _{i}\\right)}\n  \n, thus saving one multiplication in each iteration of the outer for-loop.\n\nExample\nWe are presented the linear system\n\n  \n    \n      \n        \n          \n            \n              \n                4\n                \n                  x\n                  \n                    1\n                  \n                \n                −\n                \n                  x\n                  \n                    2\n                  \n                \n                −\n                6\n                \n                  x\n                  \n                    3\n                  \n                \n                +\n                0\n                \n                  x\n                  \n                    4\n                  \n                \n              \n              \n                \n                =\n                2\n                ,\n              \n            \n            \n              \n                −\n                5\n                \n                  x\n                  \n                    1\n                  \n                \n                −\n                4\n                \n                  x\n                  \n                    2\n                  \n                \n                +\n                10\n                \n                  x\n                  \n                    3\n                  \n                \n                +\n                8\n                \n                  x\n                  \n                    4\n                  \n                \n              \n              \n                \n                =\n                21\n                ,\n              \n            \n            \n              \n                0\n                \n                  x\n                  \n                    1\n                  \n                \n                +\n                9\n                \n                  x\n                  \n                    2\n                  \n                \n                +\n                4\n                \n                  x\n                  \n                    3\n                  \n                \n                −\n                2\n                \n                  x\n                  \n                    4\n                  \n                \n              \n              \n                \n                =\n                −\n                12\n                ,\n              \n            \n            \n              \n                1\n                \n                  x\n                  \n                    1\n                  \n                \n                +\n                0\n                \n                  x\n                  \n                    2\n                  \n                \n                −\n                7\n                \n                  x\n                  \n                    3\n                  \n                \n                +\n                5\n                \n                  x\n                  \n                    4\n                  \n                \n              \n              \n                \n                =\n                −\n                6.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}4x_{1}-x_{2}-6x_{3}+0x_{4}&=2,\\\\-5x_{1}-4x_{2}+10x_{3}+8x_{4}&=21,\\\\0x_{1}+9x_{2}+4x_{3}-2x_{4}&=-12,\\\\1x_{1}+0x_{2}-7x_{3}+5x_{4}&=-6.\\end{aligned}}}\n  \n\nTo solve the equations, we choose a relaxation factor \n  \n    \n      \n        ω\n        =\n        0.5\n      \n    \n    {\\displaystyle \\omega =0.5}\n  \n and an initial guess vector \n  \n    \n      \n        ϕ\n        =\n        (\n        0\n        ,\n        0\n        ,\n        0\n        ,\n        0\n        )\n      \n    \n    {\\displaystyle \\phi =(0,0,0,0)}\n  \n. According to the successive over-relaxation algorithm, the following table is obtained, representing an exemplary iteration with approximations, which ideally, but not necessarily, finds the exact solution, (3, −2, 2, 1), in 38 steps.\n\nA simple implementation of the algorithm in Common Lisp is offered below.\n\nA simple Python implementation of the pseudo-code provided above.\n\nSymmetric successive over-relaxation\nThe version for symmetric matrices A, in which\n\n  \n    \n      \n        U\n        =\n        \n          L\n          \n            T\n          \n        \n        ,\n        \n      \n    \n    {\\displaystyle U=L^{T},\\,}\n  \n\nis referred to as Symmetric Successive Over-Relaxation, or (SSOR), in which\n\n  \n    \n      \n        P\n        =\n        \n          (\n          \n            \n              \n                D\n                ω\n              \n            \n            +\n            L\n          \n          )\n        \n        \n          \n            ω\n            \n              2\n              −\n              ω\n            \n          \n        \n        \n          D\n          \n            −\n            1\n          \n        \n        \n          (\n          \n            \n              \n                D\n                ω\n              \n            \n            +\n            U\n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle P=\\left({\\frac {D}{\\omega }}+L\\right){\\frac {\\omega }{2-\\omega }}D^{-1}\\left({\\frac {D}{\\omega }}+U\\right),}\n  \n\nand the iterative method is\n\n  \n    \n      \n        \n          \n            x\n          \n          \n            k\n            +\n            1\n          \n        \n        =\n        \n          \n            x\n          \n          \n            k\n          \n        \n        −\n        \n          γ\n          \n            k\n          \n        \n        \n          P\n          \n            −\n            1\n          \n        \n        (\n        A\n        \n          \n            x\n          \n          \n            k\n          \n        \n        −\n        \n          b\n        \n        )\n        ,\n         \n        k\n        ≥\n        0.\n      \n    \n    {\\displaystyle \\mathbf {x} ^{k+1}=\\mathbf {x} ^{k}-\\gamma ^{k}P^{-1}(A\\mathbf {x} ^{k}-\\mathbf {b} ),\\ k\\geq 0.}\n  \n\nThe SOR and SSOR methods are credited to David M. Young Jr.\n\nOther applications of the method\nA similar technique can be used for any iterative method. If the original iteration had the form\n\n  \n    \n      \n        \n          x\n          \n            n\n            +\n            1\n          \n        \n        =\n        f\n        (\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle x_{n+1}=f(x_{n})}\n  \n\nthen the modified version would use\n\n  \n    \n      \n        \n          x\n          \n            n\n            +\n            1\n          \n          \n            \n              S\n              O\n              R\n            \n          \n        \n        =\n        (\n        1\n        −\n        ω\n        )\n        \n          x\n          \n            n\n          \n          \n            \n              S\n              O\n              R\n            \n          \n        \n        +\n        ω\n        f\n        (\n        \n          x\n          \n            n\n          \n          \n            \n              S\n              O\n              R\n            \n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle x_{n+1}^{\\mathrm {SOR} }=(1-\\omega )x_{n}^{\\mathrm {SOR} }+\\omega f(x_{n}^{\\mathrm {SOR} }).}\n  \n\nHowever, the formulation presented above, used for solving systems of linear equations, is not a special case of this formulation if x is considered to be the complete vector. If this formulation is used instead, the equation for calculating the next vector will look like\n\n  \n    \n      \n        \n          \n            x\n          \n          \n            (\n            k\n            +\n            1\n            )\n          \n        \n        =\n        (\n        1\n        −\n        ω\n        )\n        \n          \n            x\n          \n          \n            (\n            k\n            )\n          \n        \n        +\n        ω\n        \n          L\n          \n            ∗\n          \n          \n            −\n            1\n          \n        \n        (\n        \n          b\n        \n        −\n        U\n        \n          \n            x\n          \n          \n            (\n            k\n            )\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle \\mathbf {x} ^{(k+1)}=(1-\\omega )\\mathbf {x} ^{(k)}+\\omega L_{*}^{-1}(\\mathbf {b} -U\\mathbf {x} ^{(k)}),}\n  \n\nwhere \n  \n    \n      \n        \n          L\n          \n            ∗\n          \n        \n        =\n        L\n        +\n        D\n      \n    \n    {\\displaystyle L_{*}=L+D}\n  \n. Values of \n  \n    \n      \n        ω\n        >\n        1\n      \n    \n    {\\displaystyle \\omega >1}\n  \n are used to speed up convergence of a slow-converging process, while values of \n  \n    \n      \n        ω\n        <\n        1\n      \n    \n    {\\displaystyle \\omega <1}\n  \n are often used to help establish convergence of a diverging iterative process or speed up the convergence of an overshooting process.\nThere are various methods that adaptively set the relaxation parameter \n  \n    \n      \n        ω\n      \n    \n    {\\displaystyle \\omega }\n  \n based on the observed behavior of the converging process. Usually they help to reach a super-linear convergence for some problems but fail for the others.\n\nSee also\nJacobi method\nGaussian Belief Propagation\nMatrix splitting\n\nNotes\nReferences\nThis article incorporates text from the article Successive_over-relaxation_method_-_SOR on CFD-Wiki that is under the GFDL license.\nAbraham Berman, Robert J. Plemmons, Nonnegative Matrices in the Mathematical Sciences, 1994, SIAM. ISBN 0-89871-321-8.\nBlack, Noel & Moore, Shirley. \"Successive Overrelaxation Method\". MathWorld.\nA. Hadjidimos, Successive overrelaxation (SOR) and related methods, Journal of Computational and Applied Mathematics 123 (2000), 177–199.\nYousef Saad, Iterative Methods for Sparse Linear Systems, 1st edition, PWS, 1996.\nNetlib's copy of  \"Templates for the Solution of Linear Systems\", by Barrett et al.\nRichard S. Varga 2002 Matrix Iterative Analysis, Second ed. (of 1962 Prentice Hall edition), Springer-Verlag.\nDavid M. Young Jr. Iterative Solution of Large Linear Systems, Academic Press, 1971. (reprinted by Dover, 2003)\n\nExternal links\nModule for the SOR Method\nTridiagonal linear system solver based on SOR, in C++\n",
  "categories": [
    "Category:Articles with example Python (programming language) code",
    "Category:Articles with example pseudocode",
    "Category:Articles with short description",
    "Category:Numerical linear algebra",
    "Category:Relaxation (iterative methods)",
    "Category:Short description matches Wikidata",
    "Category:Wikipedia articles licensed under the GNU Free Document License"
  ],
  "archived_date": "20241220_214816",
  "source_url": "https://en.wikipedia.org/wiki/Successive_over-relaxation"
}