{
  "title": "Monte Carlo integration",
  "summary": "In mathematics, Monte Carlo integration is a technique for numerical integration using random numbers. It is a particular Monte Carlo method that numerically computes a definite integral. While other algorithms usually evaluate the integrand at a regular grid, Monte Carlo randomly chooses points at which the integrand is evaluated. This method is particularly useful for higher-dimensional integrals.\nThere are different methods to perform a Monte Carlo integration, such as uniform sampling, strat",
  "content": "---\ntitle: Monte Carlo integration\nurl: https://en.wikipedia.org/wiki/Monte_Carlo_integration\nlanguage: en\ncategories: [\"Category:Articles with example C code\", \"Category:Articles with example Python (programming language) code\", \"Category:Articles with example code\", \"Category:Articles with short description\", \"Category:Monte Carlo methods\", \"Category:Short description matches Wikidata\"]\nreferences: 0\nlast_modified: 2024-12-19T13:46:33Z\n---\n\n# Monte Carlo integration\n\n## Summary\n\nIn mathematics, Monte Carlo integration is a technique for numerical integration using random numbers. It is a particular Monte Carlo method that numerically computes a definite integral. While other algorithms usually evaluate the integrand at a regular grid, Monte Carlo randomly chooses points at which the integrand is evaluated. This method is particularly useful for higher-dimensional integrals.\nThere are different methods to perform a Monte Carlo integration, such as uniform sampling, strat\n\n## Full Content\n\nIn mathematics, Monte Carlo integration is a technique for numerical integration using random numbers. It is a particular Monte Carlo method that numerically computes a definite integral. While other algorithms usually evaluate the integrand at a regular grid, Monte Carlo randomly chooses points at which the integrand is evaluated. This method is particularly useful for higher-dimensional integrals.\nThere are different methods to perform a Monte Carlo integration, such as uniform sampling, stratified sampling, importance sampling, sequential Monte Carlo (also known as a particle filter), and mean-field particle methods.\n\nOverview\nIn numerical integration, methods such as the trapezoidal rule use a deterministic approach. Monte Carlo integration, on the other hand, employs a non-deterministic approach: each realization provides a different outcome. In Monte Carlo, the final outcome is an approximation of the correct value with respective error bars, and the correct value is likely to be within those error bars.\nThe problem Monte Carlo integration addresses is the computation of a multidimensional definite integral\n\n  \n    \n      \n        I\n        =\n        \n          ∫\n          \n            Ω\n          \n        \n        f\n        (\n        \n          \n            \n              x\n            \n            ¯\n          \n        \n        )\n        \n        d\n        \n          \n            \n              x\n            \n            ¯\n          \n        \n      \n    \n    {\\displaystyle I=\\int _{\\Omega }f({\\overline {\\mathbf {x} }})\\,d{\\overline {\\mathbf {x} }}}\n  \n\nwhere Ω, a subset of \n  \n    \n      \n        \n          \n            R\n          \n          \n            m\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{m}}\n  \n, has volume\n\n  \n    \n      \n        V\n        =\n        \n          ∫\n          \n            Ω\n          \n        \n        d\n        \n          \n            \n              x\n            \n            ¯\n          \n        \n      \n    \n    {\\displaystyle V=\\int _{\\Omega }d{\\overline {\\mathbf {x} }}}\n  \n\nThe naive Monte Carlo approach is to sample points uniformly on Ω: given N uniform samples,\n\n  \n    \n      \n        \n          \n            \n              \n                x\n              \n              ¯\n            \n          \n          \n            1\n          \n        \n        ,\n        ⋯\n        ,\n        \n          \n            \n              \n                x\n              \n              ¯\n            \n          \n          \n            N\n          \n        \n        ∈\n        Ω\n        ,\n      \n    \n    {\\displaystyle {\\overline {\\mathbf {x} }}_{1},\\cdots ,{\\overline {\\mathbf {x} }}_{N}\\in \\Omega ,}\n  \n\nI can be approximated by\n\n  \n    \n      \n        I\n        ≈\n        \n          Q\n          \n            N\n          \n        \n        ≡\n        V\n        \n          \n            1\n            N\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        f\n        (\n        \n          \n            \n              \n                x\n              \n              ¯\n            \n          \n          \n            i\n          \n        \n        )\n        =\n        V\n        ⟨\n        f\n        ⟩\n        .\n      \n    \n    {\\displaystyle I\\approx Q_{N}\\equiv V{\\frac {1}{N}}\\sum _{i=1}^{N}f({\\overline {\\mathbf {x} }}_{i})=V\\langle f\\rangle .}\n  \n\nThis is because the law of large numbers ensures that\n\n  \n    \n      \n        \n          lim\n          \n            N\n            →\n            ∞\n          \n        \n        \n          Q\n          \n            N\n          \n        \n        =\n        I\n        .\n      \n    \n    {\\displaystyle \\lim _{N\\to \\infty }Q_{N}=I.}\n  \n\nGiven the estimation of I from QN, the error bars of QN can be estimated by the  sample variance using the unbiased estimate of the variance.\n\n  \n    \n      \n        \n          V\n          a\n          r\n        \n        (\n        f\n        )\n        =\n        \n          E\n        \n        (\n        \n          σ\n          \n            N\n          \n          \n            2\n          \n        \n        )\n        ≡\n        \n          \n            1\n            \n              N\n              −\n              1\n            \n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          E\n        \n        \n          [\n          \n            \n              (\n              \n                f\n                (\n                \n                  \n                    \n                      \n                        x\n                      \n                      ¯\n                    \n                  \n                  \n                    i\n                  \n                \n                )\n                −\n                ⟨\n                f\n                ⟩\n              \n              )\n            \n            \n              2\n            \n          \n          ]\n        \n        .\n      \n    \n    {\\displaystyle \\mathrm {Var} (f)=\\mathrm {E} (\\sigma _{N}^{2})\\equiv {\\frac {1}{N-1}}\\sum _{i=1}^{N}\\mathrm {E} \\left[\\left(f({\\overline {\\mathbf {x} }}_{i})-\\langle f\\rangle \\right)^{2}\\right].}\n  \n\nwhich leads to\n\n  \n    \n      \n        \n          V\n          a\n          r\n        \n        (\n        \n          Q\n          \n            N\n          \n        \n        )\n        =\n        \n          \n            \n              V\n              \n                2\n              \n            \n            \n              N\n              \n                2\n              \n            \n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          V\n          a\n          r\n        \n        (\n        f\n        )\n        =\n        \n          V\n          \n            2\n          \n        \n        \n          \n            \n              \n                V\n                a\n                r\n              \n              (\n              f\n              )\n            \n            N\n          \n        \n        =\n        \n          V\n          \n            2\n          \n        \n        \n          \n            \n              \n                E\n              \n              (\n              \n                σ\n                \n                  N\n                \n                \n                  2\n                \n              \n              )\n            \n            N\n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathrm {Var} (Q_{N})={\\frac {V^{2}}{N^{2}}}\\sum _{i=1}^{N}\\mathrm {Var} (f)=V^{2}{\\frac {\\mathrm {Var} (f)}{N}}=V^{2}{\\frac {\\mathrm {E} (\\sigma _{N}^{2})}{N}}.}\n  \n\nSince the sequence\n\n  \n    \n      \n        \n          {\n          \n            \n              E\n            \n            (\n            \n              σ\n              \n                1\n              \n              \n                2\n              \n            \n            )\n            ,\n            \n              E\n            \n            (\n            \n              σ\n              \n                2\n              \n              \n                2\n              \n            \n            )\n            ,\n            \n              E\n            \n            (\n            \n              σ\n              \n                3\n              \n              \n                2\n              \n            \n            )\n            ,\n            …\n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{\\mathrm {E} (\\sigma _{1}^{2}),\\mathrm {E} (\\sigma _{2}^{2}),\\mathrm {E} (\\sigma _{3}^{2}),\\ldots \\right\\}}\n  \n\nis bounded due to being identically equal to Var(f), as long as this is assumed finite, this variance decreases asymptotically to zero as 1/N. The estimation of the error of QN is thus\n\n  \n    \n      \n        δ\n        \n          Q\n          \n            N\n          \n        \n        ≈\n        \n          \n            \n              V\n              a\n              r\n            \n            (\n            \n              Q\n              \n                N\n              \n            \n            )\n          \n        \n        =\n        V\n        \n          \n            \n              \n                V\n                a\n                r\n              \n              (\n              f\n              )\n            \n            \n              N\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\delta Q_{N}\\approx {\\sqrt {\\mathrm {Var} (Q_{N})}}=V{\\frac {\\sqrt {\\mathrm {Var} (f)}}{\\sqrt {N}}},}\n  \n\nwhich decreases as \n  \n    \n      \n        \n          \n            \n              1\n              \n                N\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {1}{\\sqrt {N}}}}\n  \n. This is standard error of the mean multiplied with \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n. \nThis result does not depend on the number of dimensions of the integral, which is the promised advantage of Monte Carlo integration against most deterministic methods that depend exponentially on the dimension. \nIt is important to notice that, unlike in deterministic methods, the estimate of the error is not a strict error bound; random sampling may not uncover all the important features of the integrand that can result in an underestimate of the error.\nWhile the naive Monte Carlo works for simple examples, an improvement over deterministic algorithms can only be accomplished with algorithms that use problem-specific sampling distributions.\nWith an appropriate sample distribution it is possible to exploit the fact that almost all higher-dimensional integrands are very localized and only small subspace notably contributes to the integral.\nA large part of the Monte Carlo literature is dedicated in developing strategies to improve the error estimates. In particular, stratified sampling—dividing the region in sub-domains—and importance sampling—sampling from non-uniform distributions—are two examples of such techniques.\n\nExample\nA paradigmatic example of a Monte Carlo integration is the estimation of π. Consider the function\n\n  \n    \n      \n        H\n        \n          (\n          \n            x\n            ,\n            y\n          \n          )\n        \n        =\n        \n          \n            {\n            \n              \n                \n                  1\n                \n                \n                  \n                    if \n                  \n                  \n                    x\n                    \n                      2\n                    \n                  \n                  +\n                  \n                    y\n                    \n                      2\n                    \n                  \n                  ≤\n                  1\n                \n              \n              \n                \n                  0\n                \n                \n                  \n                    else\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle H\\left(x,y\\right)={\\begin{cases}1&{\\text{if }}x^{2}+y^{2}\\leq 1\\\\0&{\\text{else}}\\end{cases}}}\n  \n\nand the set Ω = [−1,1] × [−1,1] with V = 4. Notice that\n\n  \n    \n      \n        \n          I\n          \n            π\n          \n        \n        =\n        \n          ∫\n          \n            Ω\n          \n        \n        H\n        (\n        x\n        ,\n        y\n        )\n        d\n        x\n        d\n        y\n        =\n        π\n        .\n      \n    \n    {\\displaystyle I_{\\pi }=\\int _{\\Omega }H(x,y)dxdy=\\pi .}\n  \n\nThus, a crude way of calculating the value of π with Monte Carlo integration is to pick N random numbers on Ω and compute\n\n  \n    \n      \n        \n          Q\n          \n            N\n          \n        \n        =\n        4\n        \n          \n            1\n            N\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        H\n        (\n        \n          x\n          \n            i\n          \n        \n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle Q_{N}=4{\\frac {1}{N}}\\sum _{i=1}^{N}H(x_{i},y_{i})}\n  \n\nIn the figure on the right, the relative error \n  \n    \n      \n        \n          \n            \n              \n                \n                  Q\n                  \n                    N\n                  \n                \n                −\n                π\n              \n              π\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {Q_{N}-\\pi }{\\pi }}}\n  \n  is measured as a function of N, confirming the \n  \n    \n      \n        \n          \n            \n              1\n              \n                N\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {1}{\\sqrt {N}}}}\n  \n.\n\nC/C++ example\nKeep in mind that a true random number generator should be used.\n\nPython example\nMade in Python.\n\nWolfram Mathematica example\nThe code below describes a process of integrating the function\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              1\n              +\n              sinh\n              ⁡\n              (\n              2\n              x\n              )\n              log\n              ⁡\n              (\n              x\n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(x)={\\frac {1}{1+\\sinh(2x)\\log(x)^{2}}}}\n  \n\nfrom \n  \n    \n      \n        0.8\n        <\n        x\n        <\n        3\n      \n    \n    {\\displaystyle 0.8<x<3}\n  \n using the Monte-Carlo method in Mathematica:\n\nRecursive stratified sampling\nRecursive stratified sampling is a generalization of one-dimensional adaptive quadratures to multi-dimensional integrals. On each recursion step the integral and the error are estimated using a plain Monte Carlo algorithm. If the error estimate is larger than the required accuracy the integration volume is divided into sub-volumes and the procedure is recursively applied to sub-volumes.\nThe ordinary 'dividing by two' strategy does not work for multi-dimensions as the number of sub-volumes grows far too quickly to keep track. Instead one estimates along which dimension a subdivision should bring the most dividends and only subdivides the volume along this dimension.\nThe stratified sampling algorithm concentrates the sampling points in the regions where the variance of the function is largest thus reducing the grand variance and making the sampling more effective, as shown on the illustration.\nThe popular MISER routine implements a similar algorithm.\n\nMISER Monte Carlo\nThe MISER algorithm is based on recursive stratified sampling. This technique aims to reduce the overall integration error by concentrating integration points in the regions of highest variance.\nThe idea of stratified sampling begins with the observation that for two disjoint regions a and b with Monte Carlo estimates of the integral \n  \n    \n      \n        \n          E\n          \n            a\n          \n        \n        (\n        f\n        )\n      \n    \n    {\\displaystyle E_{a}(f)}\n  \n and \n  \n    \n      \n        \n          E\n          \n            b\n          \n        \n        (\n        f\n        )\n      \n    \n    {\\displaystyle E_{b}(f)}\n  \n and variances \n  \n    \n      \n        \n          σ\n          \n            a\n          \n          \n            2\n          \n        \n        (\n        f\n        )\n      \n    \n    {\\displaystyle \\sigma _{a}^{2}(f)}\n  \n and \n  \n    \n      \n        \n          σ\n          \n            b\n          \n          \n            2\n          \n        \n        (\n        f\n        )\n      \n    \n    {\\displaystyle \\sigma _{b}^{2}(f)}\n  \n, the variance Var(f) of the combined estimate\n\n  \n    \n      \n        E\n        (\n        f\n        )\n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        \n          (\n          \n            \n              E\n              \n                a\n              \n            \n            (\n            f\n            )\n            +\n            \n              E\n              \n                b\n              \n            \n            (\n            f\n            )\n          \n          )\n        \n      \n    \n    {\\displaystyle E(f)={\\tfrac {1}{2}}\\left(E_{a}(f)+E_{b}(f)\\right)}\n  \n\nis given by,\n\n  \n    \n      \n        \n          V\n          a\n          r\n        \n        (\n        f\n        )\n        =\n        \n          \n            \n              \n                σ\n                \n                  a\n                \n                \n                  2\n                \n              \n              (\n              f\n              )\n            \n            \n              4\n              \n                N\n                \n                  a\n                \n              \n            \n          \n        \n        +\n        \n          \n            \n              \n                σ\n                \n                  b\n                \n                \n                  2\n                \n              \n              (\n              f\n              )\n            \n            \n              4\n              \n                N\n                \n                  b\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {Var} (f)={\\frac {\\sigma _{a}^{2}(f)}{4N_{a}}}+{\\frac {\\sigma _{b}^{2}(f)}{4N_{b}}}}\n  \n\nIt can be shown that this variance is minimized by distributing the points such that,\n\n  \n    \n      \n        \n          \n            \n              N\n              \n                a\n              \n            \n            \n              \n                N\n                \n                  a\n                \n              \n              +\n              \n                N\n                \n                  b\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              σ\n              \n                a\n              \n            \n            \n              \n                σ\n                \n                  a\n                \n              \n              +\n              \n                σ\n                \n                  b\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {N_{a}}{N_{a}+N_{b}}}={\\frac {\\sigma _{a}}{\\sigma _{a}+\\sigma _{b}}}}\n  \n\nHence the smallest error estimate is obtained by allocating sample points in proportion to the standard deviation of the function in each sub-region.\nThe MISER algorithm proceeds by bisecting the integration region along one coordinate axis to give two sub-regions at each step. The direction is chosen by examining all d possible bisections and selecting the one which will minimize the combined variance of the two sub-regions. The variance in the sub-regions is estimated by sampling with a fraction of the total number of points available to the current step. The same procedure is then repeated recursively for each of the two half-spaces from the best bisection. The remaining sample points are allocated to the sub-regions using the formula for Na and Nb. This recursive allocation of integration points continues down to a user-specified depth where each sub-region is integrated using a plain Monte Carlo estimate. These individual values and their error estimates are then combined upwards to give an overall result and an estimate of its error.\n\nImportance sampling\nThere are a variety of importance sampling algorithms, such as\n\nImportance sampling algorithm\nImportance sampling provides a very important tool to perform Monte-Carlo integration. The main result of importance sampling to this method is that the uniform sampling of \n  \n    \n      \n        \n          \n            \n              x\n            \n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {\\mathbf {x} }}}\n  \n is a particular case of a more generic choice, on which the samples are drawn from any distribution \n  \n    \n      \n        p\n        (\n        \n          \n            \n              x\n            \n            ¯\n          \n        \n        )\n      \n    \n    {\\displaystyle p({\\overline {\\mathbf {x} }})}\n  \n. The idea is that \n  \n    \n      \n        p\n        (\n        \n          \n            \n              x\n            \n            ¯\n          \n        \n        )\n      \n    \n    {\\displaystyle p({\\overline {\\mathbf {x} }})}\n  \n can be chosen to decrease the variance of the measurement QN.\nConsider the following example where one would like to numerically integrate a gaussian function, centered at 0, with σ = 1, from −1000 to 1000. Naturally, if the samples are drawn uniformly on the interval [−1000, 1000], only a very small part of them would be significant to the integral. This can be improved by choosing a different distribution from where the samples are chosen, for instance by sampling according to a gaussian distribution centered at 0, with σ = 1. Of course the \"right\" choice strongly depends on the integrand.\nFormally, given a set of samples chosen from a distribution\n\n  \n    \n      \n        p\n        (\n        \n          \n            \n              x\n            \n            ¯\n          \n        \n        )\n        :\n        \n        \n          \n            \n              \n                x\n              \n              ¯\n            \n          \n          \n            1\n          \n        \n        ,\n        ⋯\n        ,\n        \n          \n            \n              \n                x\n              \n              ¯\n            \n          \n          \n            N\n          \n        \n        ∈\n        V\n        ,\n      \n    \n    {\\displaystyle p({\\overline {\\mathbf {x} }}):\\qquad {\\overline {\\mathbf {x} }}_{1},\\cdots ,{\\overline {\\mathbf {x} }}_{N}\\in V,}\n  \n\nthe estimator for I is given by\n\n  \n    \n      \n        \n          Q\n          \n            N\n          \n        \n        ≡\n        \n          \n            1\n            N\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          \n            \n              f\n              (\n              \n                \n                  \n                    \n                      x\n                    \n                    ¯\n                  \n                \n                \n                  i\n                \n              \n              )\n            \n            \n              p\n              (\n              \n                \n                  \n                    \n                      x\n                    \n                    ¯\n                  \n                \n                \n                  i\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle Q_{N}\\equiv {\\frac {1}{N}}\\sum _{i=1}^{N}{\\frac {f({\\overline {\\mathbf {x} }}_{i})}{p({\\overline {\\mathbf {x} }}_{i})}}}\n  \n\nIntuitively, this says that if we pick a particular sample twice as much as other samples, we weight it half as much as the other samples. This estimator is naturally valid for uniform sampling, the case where \n  \n    \n      \n        p\n        (\n        \n          \n            \n              x\n            \n            ¯\n          \n        \n        )\n      \n    \n    {\\displaystyle p({\\overline {\\mathbf {x} }})}\n  \n is constant.\nThe Metropolis–Hastings algorithm is one of the most used algorithms to generate \n  \n    \n      \n        \n          \n            \n              x\n            \n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {\\mathbf {x} }}}\n  \n from \n  \n    \n      \n        p\n        (\n        \n          \n            \n              x\n            \n            ¯\n          \n        \n        )\n      \n    \n    {\\displaystyle p({\\overline {\\mathbf {x} }})}\n  \n, thus providing an efficient way of computing integrals.\n\nVEGAS Monte Carlo\nThe VEGAS algorithm approximates the exact distribution by making a number of passes over the integration region which creates the histogram of the function f. Each histogram is used to define a sampling distribution for the next pass. Asymptotically this procedure converges to the desired distribution. In order to avoid the number of histogram bins growing like Kd, the probability distribution is approximated by a separable function:\n\n  \n    \n      \n        g\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        …\n        )\n        =\n        \n          g\n          \n            1\n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        )\n        \n          g\n          \n            2\n          \n        \n        (\n        \n          x\n          \n            2\n          \n        \n        )\n        …\n      \n    \n    {\\displaystyle g(x_{1},x_{2},\\ldots )=g_{1}(x_{1})g_{2}(x_{2})\\ldots }\n  \n\nso that the number of bins required is only Kd. This is equivalent to locating the peaks of the function from the projections of the integrand onto the coordinate axes. The efficiency of VEGAS depends on the validity of this assumption. It is most efficient when the peaks of the integrand are well-localized. If an integrand can be rewritten in a form which is approximately separable this will increase the efficiency of integration with VEGAS. VEGAS incorporates a number of additional features, and combines both stratified sampling and importance sampling.\n\nSee also\nQuasi-Monte Carlo method\nAuxiliary field Monte Carlo\nMonte Carlo method in statistical physics\nMonte Carlo method\nVariance reduction\n\nNotes\nReferences\nExternal links\nCafé math : Monte Carlo Integration : A blog article describing Monte Carlo integration (principle, hypothesis, confidence interval)\nBoost.Math : Naive Monte Carlo integration: Documentation for the C++ naive Monte-Carlo routines\nMonte Carlo applet applied in statistical physics problems\n",
  "categories": [
    "Category:Articles with example C code",
    "Category:Articles with example Python (programming language) code",
    "Category:Articles with example code",
    "Category:Articles with short description",
    "Category:Monte Carlo methods",
    "Category:Short description matches Wikidata"
  ],
  "archived_date": "20241221_183012",
  "source_url": "https://en.wikipedia.org/wiki/Monte_Carlo_integration"
}