{
  "title": "Inverse Gaussian distribution",
  "summary": "In probability theory, the inverse Gaussian distribution (also known as the Wald distribution) is a two-parameter family of continuous probability distributions with support on (0,∞).\nIts probability density function is given by\n\n  \n    \n      \n        f\n        (\n        x\n        ;\n        μ\n        ,\n        λ\n        )\n        =\n        \n          \n            \n              λ\n              \n                2\n                π\n                \n                  x\n                  \n         ",
  "content": "---\ntitle: Inverse Gaussian distribution\nurl: https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution\nlanguage: en\ncategories: [\"Category:Articles contradicting other articles\", \"Category:Articles with example Java code\", \"Category:Articles with example Python (programming language) code\", \"Category:Articles with short description\", \"Category:CS1: long volume value\", \"Category:CS1 French-language sources (fr)\", \"Category:CS1 German-language sources (de)\", \"Category:Continuous distributions\", \"Category:Exponential family distributions\", \"Category:Infinitely divisible probability distributions\", \"Category:Short description matches Wikidata\"]\nreferences: 0\nlast_modified: 2024-12-19T13:56:05Z\n---\n\n# Inverse Gaussian distribution\n\n## Summary\n\nIn probability theory, the inverse Gaussian distribution (also known as the Wald distribution) is a two-parameter family of continuous probability distributions with support on (0,∞).\nIts probability density function is given by\n\n  \n    \n      \n        f\n        (\n        x\n        ;\n        μ\n        ,\n        λ\n        )\n        =\n        \n          \n            \n              λ\n              \n                2\n                π\n                \n                  x\n                  \n         \n\n## Full Content\n\nIn probability theory, the inverse Gaussian distribution (also known as the Wald distribution) is a two-parameter family of continuous probability distributions with support on (0,∞).\nIts probability density function is given by\n\n  \n    \n      \n        f\n        (\n        x\n        ;\n        μ\n        ,\n        λ\n        )\n        =\n        \n          \n            \n              λ\n              \n                2\n                π\n                \n                  x\n                  \n                    3\n                  \n                \n              \n            \n          \n        \n        exp\n        ⁡\n        \n          \n            (\n          \n        \n        −\n        \n          \n            \n              λ\n              (\n              x\n              −\n              μ\n              \n                )\n                \n                  2\n                \n              \n            \n            \n              2\n              \n                μ\n                \n                  2\n                \n              \n              x\n            \n          \n        \n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle f(x;\\mu ,\\lambda )={\\sqrt {\\frac {\\lambda }{2\\pi x^{3}}}}\\exp {\\biggl (}-{\\frac {\\lambda (x-\\mu )^{2}}{2\\mu ^{2}x}}{\\biggr )}}\n  \n\nfor x > 0, where \n  \n    \n      \n        μ\n        >\n        0\n      \n    \n    {\\displaystyle \\mu >0}\n  \n is the mean and \n  \n    \n      \n        λ\n        >\n        0\n      \n    \n    {\\displaystyle \\lambda >0}\n  \n is the shape parameter.\nThe inverse Gaussian distribution has several properties analogous to a Gaussian distribution.  The name can be misleading:  it is an \"inverse\" only in that, while the Gaussian describes a Brownian motion's level at a fixed time, the inverse Gaussian describes the distribution of the time a Brownian motion with positive drift takes to reach a fixed positive level.\nIts cumulant generating function (logarithm of the characteristic function) is the inverse of the cumulant generating function of a Gaussian random variable.\nTo indicate that a random variable X is inverse Gaussian-distributed with mean μ and shape parameter λ we write \n  \n    \n      \n        X\n        ∼\n        IG\n        ⁡\n        (\n        μ\n        ,\n        λ\n        )\n        \n        \n      \n    \n    {\\displaystyle X\\sim \\operatorname {IG} (\\mu ,\\lambda )\\,\\!}\n  \n.\n\nProperties\nSingle parameter form\nThe probability density function (pdf) of the inverse Gaussian distribution has a single parameter form given by\n\n  \n    \n      \n        f\n        (\n        x\n        ;\n        μ\n        ,\n        \n          μ\n          \n            2\n          \n        \n        )\n        =\n        \n          \n            μ\n            \n              2\n              π\n              \n                x\n                \n                  3\n                \n              \n            \n          \n        \n        exp\n        ⁡\n        \n          \n            (\n          \n        \n        −\n        \n          \n            \n              (\n              x\n              −\n              μ\n              \n                )\n                \n                  2\n                \n              \n            \n            \n              2\n              x\n            \n          \n        \n        \n          \n            )\n          \n        \n        .\n      \n    \n    {\\displaystyle f(x;\\mu ,\\mu ^{2})={\\frac {\\mu }{\\sqrt {2\\pi x^{3}}}}\\exp {\\biggl (}-{\\frac {(x-\\mu )^{2}}{2x}}{\\biggr )}.}\n  \n\nIn this form, the mean and variance of the distribution are equal, \n  \n    \n      \n        \n          E\n        \n        [\n        X\n        ]\n        =\n        \n          Var\n        \n        (\n        X\n        )\n        .\n      \n    \n    {\\displaystyle \\mathbb {E} [X]={\\text{Var}}(X).}\n  \n\nAlso, the cumulative distribution function (cdf) of the single parameter inverse Gaussian distribution is related to the standard normal distribution by\n\n  \n    \n      \n        \n          \n            \n              \n                Pr\n                (\n                X\n                <\n                x\n                )\n              \n              \n                \n                =\n                Φ\n                (\n                −\n                \n                  z\n                  \n                    1\n                  \n                \n                )\n                +\n                \n                  e\n                  \n                    2\n                    μ\n                  \n                \n                Φ\n                (\n                −\n                \n                  z\n                  \n                    2\n                  \n                \n                )\n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\Pr(X<x)&=\\Phi (-z_{1})+e^{2\\mu }\\Phi (-z_{2}),\\end{aligned}}}\n  \n\nwhere \n  \n    \n      \n        \n          z\n          \n            1\n          \n        \n        =\n        \n          \n            μ\n            \n              x\n              \n                1\n                \n                  /\n                \n                2\n              \n            \n          \n        \n        −\n        \n          x\n          \n            1\n            \n              /\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle z_{1}={\\frac {\\mu }{x^{1/2}}}-x^{1/2}}\n  \n, \n  \n    \n      \n        \n          z\n          \n            2\n          \n        \n        =\n        \n          \n            μ\n            \n              x\n              \n                1\n                \n                  /\n                \n                2\n              \n            \n          \n        \n        +\n        \n          x\n          \n            1\n            \n              /\n            \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle z_{2}={\\frac {\\mu }{x^{1/2}}}+x^{1/2},}\n  \n and the \n  \n    \n      \n        Φ\n      \n    \n    {\\displaystyle \\Phi }\n  \n is the cdf of standard normal distribution. The variables \n  \n    \n      \n        \n          z\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle z_{1}}\n  \n and \n  \n    \n      \n        \n          z\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle z_{2}}\n  \n are related to each other by the identity \n  \n    \n      \n        \n          z\n          \n            2\n          \n          \n            2\n          \n        \n        =\n        \n          z\n          \n            1\n          \n          \n            2\n          \n        \n        +\n        4\n        μ\n        .\n      \n    \n    {\\displaystyle z_{2}^{2}=z_{1}^{2}+4\\mu .}\n  \n\nIn the single parameter form, the MGF simplifies to\n\n  \n    \n      \n        M\n        (\n        t\n        )\n        =\n        exp\n        ⁡\n        [\n        μ\n        (\n        1\n        −\n        \n          \n            1\n            −\n            2\n            t\n          \n        \n        )\n        ]\n        .\n      \n    \n    {\\displaystyle M(t)=\\exp[\\mu (1-{\\sqrt {1-2t}})].}\n  \n\nAn inverse Gaussian distribution in double parameter form \n  \n    \n      \n        f\n        (\n        x\n        ;\n        μ\n        ,\n        λ\n        )\n      \n    \n    {\\displaystyle f(x;\\mu ,\\lambda )}\n  \n can be transformed into a single parameter form \n  \n    \n      \n        f\n        (\n        y\n        ;\n        \n          μ\n          \n            0\n          \n        \n        ,\n        \n          μ\n          \n            0\n          \n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle f(y;\\mu _{0},\\mu _{0}^{2})}\n  \n by appropriate scaling \n  \n    \n      \n        y\n        =\n        \n          \n            \n              \n                μ\n                \n                  2\n                \n              \n              x\n            \n            λ\n          \n        \n        ,\n      \n    \n    {\\displaystyle y={\\frac {\\mu ^{2}x}{\\lambda }},}\n  \n where \n  \n    \n      \n        \n          μ\n          \n            0\n          \n        \n        =\n        \n          μ\n          \n            3\n          \n        \n        \n          /\n        \n        λ\n        .\n      \n    \n    {\\displaystyle \\mu _{0}=\\mu ^{3}/\\lambda .}\n  \n\nThe standard form of inverse Gaussian distribution is\n\n  \n    \n      \n        f\n        (\n        x\n        ;\n        1\n        ,\n        1\n        )\n        =\n        \n          \n            1\n            \n              2\n              π\n              \n                x\n                \n                  3\n                \n              \n            \n          \n        \n        exp\n        ⁡\n        \n          \n            (\n          \n        \n        −\n        \n          \n            \n              (\n              x\n              −\n              1\n              \n                )\n                \n                  2\n                \n              \n            \n            \n              2\n              x\n            \n          \n        \n        \n          \n            )\n          \n        \n        .\n      \n    \n    {\\displaystyle f(x;1,1)={\\frac {1}{\\sqrt {2\\pi x^{3}}}}\\exp {\\biggl (}-{\\frac {(x-1)^{2}}{2x}}{\\biggr )}.}\n\nSummation\nIf Xi has an \n  \n    \n      \n        IG\n        ⁡\n        (\n        \n          μ\n          \n            0\n          \n        \n        \n          w\n          \n            i\n          \n        \n        ,\n        \n          λ\n          \n            0\n          \n        \n        \n          w\n          \n            i\n          \n          \n            2\n          \n        \n        )\n        \n        \n      \n    \n    {\\displaystyle \\operatorname {IG} (\\mu _{0}w_{i},\\lambda _{0}w_{i}^{2})\\,\\!}\n  \n distribution for i = 1, 2, ..., n\nand all Xi are independent, then\n\n  \n    \n      \n        S\n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          X\n          \n            i\n          \n        \n        ∼\n        IG\n        ⁡\n        \n          (\n          \n            \n              μ\n              \n                0\n              \n            \n            ∑\n            \n              w\n              \n                i\n              \n            \n            ,\n            \n              λ\n              \n                0\n              \n            \n            \n              \n                (\n                \n                  ∑\n                  \n                    w\n                    \n                      i\n                    \n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle S=\\sum _{i=1}^{n}X_{i}\\sim \\operatorname {IG} \\left(\\mu _{0}\\sum w_{i},\\lambda _{0}\\left(\\sum w_{i}\\right)^{2}\\right).}\n  \n\nNote that\n\n  \n    \n      \n        \n          \n            \n              Var\n              ⁡\n              (\n              \n                X\n                \n                  i\n                \n              \n              )\n            \n            \n              E\n              ⁡\n              (\n              \n                X\n                \n                  i\n                \n              \n              )\n            \n          \n        \n        =\n        \n          \n            \n              \n                μ\n                \n                  0\n                \n                \n                  2\n                \n              \n              \n                w\n                \n                  i\n                \n                \n                  2\n                \n              \n            \n            \n              \n                λ\n                \n                  0\n                \n              \n              \n                w\n                \n                  i\n                \n                \n                  2\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              μ\n              \n                0\n              \n              \n                2\n              \n            \n            \n              λ\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\operatorname {Var} (X_{i})}{\\operatorname {E} (X_{i})}}={\\frac {\\mu _{0}^{2}w_{i}^{2}}{\\lambda _{0}w_{i}^{2}}}={\\frac {\\mu _{0}^{2}}{\\lambda _{0}}}}\n  \n\nis constant for all i. This is a necessary condition for the summation. Otherwise S would not be Inverse Gaussian distributed.\n\nScaling\nFor any t > 0 it holds that\n\n  \n    \n      \n        X\n        ∼\n        IG\n        ⁡\n        (\n        μ\n        ,\n        λ\n        )\n        \n        \n        \n        \n        \n        \n        ⇒\n        \n        \n        \n        \n        \n        \n        t\n        X\n        ∼\n        IG\n        ⁡\n        (\n        t\n        μ\n        ,\n        t\n        λ\n        )\n        .\n      \n    \n    {\\displaystyle X\\sim \\operatorname {IG} (\\mu ,\\lambda )\\,\\,\\,\\,\\,\\,\\Rightarrow \\,\\,\\,\\,\\,\\,tX\\sim \\operatorname {IG} (t\\mu ,t\\lambda ).}\n\nExponential family\nThe inverse Gaussian distribution is a two-parameter exponential family with natural parameters −λ/(2μ2) and −λ/2, and natural statistics X and 1/X.\nFor \n  \n    \n      \n        λ\n        >\n        0\n      \n    \n    {\\displaystyle \\lambda >0}\n  \n fixed, it is also a single-parameter natural exponential family distribution where the base distribution has density\n\n  \n    \n      \n        h\n        (\n        x\n        )\n        =\n        \n          \n            \n              λ\n              \n                2\n                π\n                \n                  x\n                  \n                    3\n                  \n                \n              \n            \n          \n        \n        exp\n        ⁡\n        \n          (\n          \n            −\n            \n              \n                λ\n                \n                  2\n                  x\n                \n              \n            \n          \n          )\n        \n        \n          \n            1\n          \n          \n            [\n            0\n            ,\n            ∞\n            )\n          \n        \n        (\n        x\n        )\n        \n        .\n      \n    \n    {\\displaystyle h(x)={\\sqrt {\\frac {\\lambda }{2\\pi x^{3}}}}\\exp \\left(-{\\frac {\\lambda }{2x}}\\right)\\mathbb {1} _{[0,\\infty )}(x)\\,.}\n  \n\nIndeed, with \n  \n    \n      \n        θ\n        ≤\n        0\n      \n    \n    {\\displaystyle \\theta \\leq 0}\n  \n,\n\n  \n    \n      \n        p\n        (\n        x\n        ;\n        θ\n        )\n        =\n        \n          \n            \n              exp\n              ⁡\n              (\n              θ\n              x\n              )\n              h\n              (\n              x\n              )\n            \n            \n              ∫\n              exp\n              ⁡\n              (\n              θ\n              y\n              )\n              h\n              (\n              y\n              )\n              d\n              y\n            \n          \n        \n      \n    \n    {\\displaystyle p(x;\\theta )={\\frac {\\exp(\\theta x)h(x)}{\\int \\exp(\\theta y)h(y)dy}}}\n  \n\nis a density over the reals. Evaluating the integral, we get\n\n  \n    \n      \n        p\n        (\n        x\n        ;\n        θ\n        )\n        =\n        \n          \n            \n              λ\n              \n                2\n                π\n                \n                  x\n                  \n                    3\n                  \n                \n              \n            \n          \n        \n        exp\n        ⁡\n        \n          (\n          \n            −\n            \n              \n                λ\n                \n                  2\n                  x\n                \n              \n            \n            +\n            θ\n            x\n            −\n            \n              \n                −\n                2\n                λ\n                θ\n              \n            \n          \n          )\n        \n        \n          \n            1\n          \n          \n            [\n            0\n            ,\n            ∞\n            )\n          \n        \n        (\n        x\n        )\n        \n        .\n      \n    \n    {\\displaystyle p(x;\\theta )={\\sqrt {\\frac {\\lambda }{2\\pi x^{3}}}}\\exp \\left(-{\\frac {\\lambda }{2x}}+\\theta x-{\\sqrt {-2\\lambda \\theta }}\\right)\\mathbb {1} _{[0,\\infty )}(x)\\,.}\n  \n\nSubstituting \n  \n    \n      \n        θ\n        =\n        −\n        λ\n        \n          /\n        \n        (\n        2\n        \n          μ\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle \\theta =-\\lambda /(2\\mu ^{2})}\n  \n makes the above expression equal to \n  \n    \n      \n        f\n        (\n        x\n        ;\n        μ\n        ,\n        λ\n        )\n      \n    \n    {\\displaystyle f(x;\\mu ,\\lambda )}\n  \n.\n\nRelationship with Brownian motion\nLet the stochastic process Xt be given by\n\n  \n    \n      \n        \n          X\n          \n            0\n          \n        \n        =\n        0\n        \n      \n    \n    {\\displaystyle X_{0}=0\\quad }\n  \n\n  \n    \n      \n        \n          X\n          \n            t\n          \n        \n        =\n        ν\n        t\n        +\n        σ\n        \n          W\n          \n            t\n          \n        \n        \n        \n        \n        \n      \n    \n    {\\displaystyle X_{t}=\\nu t+\\sigma W_{t}\\quad \\quad \\quad \\quad }\n  \n\nwhere Wt is a standard Brownian motion. That is, Xt is a Brownian motion with drift \n  \n    \n      \n        ν\n        >\n        0\n      \n    \n    {\\displaystyle \\nu >0}\n  \n.\nThen the first passage time for a fixed level \n  \n    \n      \n        α\n        >\n        0\n      \n    \n    {\\displaystyle \\alpha >0}\n  \n by Xt is distributed according to an inverse-Gaussian:\n\n  \n    \n      \n        \n          T\n          \n            α\n          \n        \n        =\n        inf\n        {\n        t\n        >\n        0\n        ∣\n        \n          X\n          \n            t\n          \n        \n        =\n        α\n        }\n        ∼\n        IG\n        ⁡\n        \n          (\n          \n            \n              \n                α\n                ν\n              \n            \n            ,\n            \n              \n                (\n                \n                  \n                    α\n                    σ\n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n          )\n        \n        =\n        \n          \n            α\n            \n              σ\n              \n                \n                  2\n                  π\n                  \n                    x\n                    \n                      3\n                    \n                  \n                \n              \n            \n          \n        \n        exp\n        ⁡\n        \n          \n            (\n          \n        \n        −\n        \n          \n            \n              (\n              α\n              −\n              ν\n              x\n              \n                )\n                \n                  2\n                \n              \n            \n            \n              2\n              \n                σ\n                \n                  2\n                \n              \n              x\n            \n          \n        \n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle T_{\\alpha }=\\inf\\{t>0\\mid X_{t}=\\alpha \\}\\sim \\operatorname {IG} \\left({\\frac {\\alpha }{\\nu }},\\left({\\frac {\\alpha }{\\sigma }}\\right)^{2}\\right)={\\frac {\\alpha }{\\sigma {\\sqrt {2\\pi x^{3}}}}}\\exp {\\biggl (}-{\\frac {(\\alpha -\\nu x)^{2}}{2\\sigma ^{2}x}}{\\biggr )}}\n  \n\ni.e\n\n  \n    \n      \n        P\n        (\n        \n          T\n          \n            α\n          \n        \n        ∈\n        (\n        T\n        ,\n        T\n        +\n        d\n        T\n        )\n        )\n        =\n        \n          \n            α\n            \n              σ\n              \n                \n                  2\n                  π\n                  \n                    T\n                    \n                      3\n                    \n                  \n                \n              \n            \n          \n        \n        exp\n        ⁡\n        \n          \n            (\n          \n        \n        −\n        \n          \n            \n              (\n              α\n              −\n              ν\n              T\n              \n                )\n                \n                  2\n                \n              \n            \n            \n              2\n              \n                σ\n                \n                  2\n                \n              \n              T\n            \n          \n        \n        \n          \n            )\n          \n        \n        d\n        T\n      \n    \n    {\\displaystyle P(T_{\\alpha }\\in (T,T+dT))={\\frac {\\alpha }{\\sigma {\\sqrt {2\\pi T^{3}}}}}\\exp {\\biggl (}-{\\frac {(\\alpha -\\nu T)^{2}}{2\\sigma ^{2}T}}{\\biggr )}dT}\n  \n\n(cf. Schrödinger equation 19, Smoluchowski, equation 8, and Folks, equation 1).\n\nWhen drift is zero\nA common special case of the above arises when the Brownian motion has no drift.  In that case, parameter μ tends to infinity, and the first passage time for fixed level α has probability density function\n\n  \n    \n      \n        f\n        \n          (\n          \n            x\n            ;\n            0\n            ,\n            \n              \n                (\n                \n                  \n                    α\n                    σ\n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n          )\n        \n        =\n        \n          \n            α\n            \n              σ\n              \n                \n                  2\n                  π\n                  \n                    x\n                    \n                      3\n                    \n                  \n                \n              \n            \n          \n        \n        exp\n        ⁡\n        \n          (\n          \n            −\n            \n              \n                \n                  α\n                  \n                    2\n                  \n                \n                \n                  2\n                  \n                    σ\n                    \n                      2\n                    \n                  \n                  x\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle f\\left(x;0,\\left({\\frac {\\alpha }{\\sigma }}\\right)^{2}\\right)={\\frac {\\alpha }{\\sigma {\\sqrt {2\\pi x^{3}}}}}\\exp \\left(-{\\frac {\\alpha ^{2}}{2\\sigma ^{2}x}}\\right)}\n  \n\n(see also Bachelier: 74 : 39 ). This is a Lévy distribution with parameters \n  \n    \n      \n        c\n        =\n        \n          \n            (\n            \n              \n                α\n                σ\n              \n            \n            )\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle c=\\left({\\frac {\\alpha }{\\sigma }}\\right)^{2}}\n  \n and \n  \n    \n      \n        μ\n        =\n        0\n      \n    \n    {\\displaystyle \\mu =0}\n  \n.\n\nMaximum likelihood\nThe model where\n\n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n        ∼\n        IG\n        ⁡\n        (\n        μ\n        ,\n        λ\n        \n          w\n          \n            i\n          \n        \n        )\n        ,\n        \n        \n        \n        \n        \n        \n        i\n        =\n        1\n        ,\n        2\n        ,\n        …\n        ,\n        n\n      \n    \n    {\\displaystyle X_{i}\\sim \\operatorname {IG} (\\mu ,\\lambda w_{i}),\\,\\,\\,\\,\\,\\,i=1,2,\\ldots ,n}\n  \n\nwith all wi known, (μ, λ) unknown and all Xi independent has the following likelihood function\n\n  \n    \n      \n        L\n        (\n        μ\n        ,\n        λ\n        )\n        =\n        \n          \n            (\n            \n              \n                λ\n                \n                  2\n                  π\n                \n              \n            \n            )\n          \n          \n            \n              n\n              2\n            \n          \n        \n        \n          \n            (\n            \n              \n                ∏\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                \n                  \n                    w\n                    \n                      i\n                    \n                  \n                  \n                    X\n                    \n                      i\n                    \n                    \n                      3\n                    \n                  \n                \n              \n            \n            )\n          \n          \n            \n              1\n              2\n            \n          \n        \n        exp\n        ⁡\n        \n          (\n          \n            \n              \n                λ\n                μ\n              \n            \n            \n              ∑\n              \n                i\n                =\n                1\n              \n              \n                n\n              \n            \n            \n              w\n              \n                i\n              \n            \n            −\n            \n              \n                λ\n                \n                  2\n                  \n                    μ\n                    \n                      2\n                    \n                  \n                \n              \n            \n            \n              ∑\n              \n                i\n                =\n                1\n              \n              \n                n\n              \n            \n            \n              w\n              \n                i\n              \n            \n            \n              X\n              \n                i\n              \n            \n            −\n            \n              \n                λ\n                2\n              \n            \n            \n              ∑\n              \n                i\n                =\n                1\n              \n              \n                n\n              \n            \n            \n              w\n              \n                i\n              \n            \n            \n              \n                1\n                \n                  X\n                  \n                    i\n                  \n                \n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle L(\\mu ,\\lambda )=\\left({\\frac {\\lambda }{2\\pi }}\\right)^{\\frac {n}{2}}\\left(\\prod _{i=1}^{n}{\\frac {w_{i}}{X_{i}^{3}}}\\right)^{\\frac {1}{2}}\\exp \\left({\\frac {\\lambda }{\\mu }}\\sum _{i=1}^{n}w_{i}-{\\frac {\\lambda }{2\\mu ^{2}}}\\sum _{i=1}^{n}w_{i}X_{i}-{\\frac {\\lambda }{2}}\\sum _{i=1}^{n}w_{i}{\\frac {1}{X_{i}}}\\right).}\n  \n\nSolving the likelihood equation yields the following maximum likelihood estimates\n\n  \n    \n      \n        \n          \n            \n              μ\n              ^\n            \n          \n        \n        =\n        \n          \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                w\n                \n                  i\n                \n              \n              \n                X\n                \n                  i\n                \n              \n            \n            \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                w\n                \n                  i\n                \n              \n            \n          \n        \n        ,\n        \n        \n        \n        \n        \n        \n        \n        \n        \n          \n            1\n            \n              \n                λ\n                ^\n              \n            \n          \n        \n        =\n        \n          \n            1\n            n\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          w\n          \n            i\n          \n        \n        \n          (\n          \n            \n              \n                1\n                \n                  X\n                  \n                    i\n                  \n                \n              \n            \n            −\n            \n              \n                1\n                \n                  \n                    μ\n                    ^\n                  \n                \n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle {\\widehat {\\mu }}={\\frac {\\sum _{i=1}^{n}w_{i}X_{i}}{\\sum _{i=1}^{n}w_{i}}},\\,\\,\\,\\,\\,\\,\\,\\,{\\frac {1}{\\widehat {\\lambda }}}={\\frac {1}{n}}\\sum _{i=1}^{n}w_{i}\\left({\\frac {1}{X_{i}}}-{\\frac {1}{\\widehat {\\mu }}}\\right).}\n  \n\n  \n    \n      \n        \n          \n            \n              μ\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\mu }}}\n  \n and \n  \n    \n      \n        \n          \n            \n              λ\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\lambda }}}\n  \n are independent and\n\n  \n    \n      \n        \n          \n            \n              μ\n              ^\n            \n          \n        \n        ∼\n        IG\n        ⁡\n        \n          (\n          \n            μ\n            ,\n            λ\n            \n              ∑\n              \n                i\n                =\n                1\n              \n              \n                n\n              \n            \n            \n              w\n              \n                i\n              \n            \n          \n          )\n        \n        ,\n        \n        \n          \n            n\n            \n              \n                λ\n                ^\n              \n            \n          \n        \n        ∼\n        \n          \n            1\n            λ\n          \n        \n        \n          χ\n          \n            n\n            −\n            1\n          \n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\widehat {\\mu }}\\sim \\operatorname {IG} \\left(\\mu ,\\lambda \\sum _{i=1}^{n}w_{i}\\right),\\qquad {\\frac {n}{\\widehat {\\lambda }}}\\sim {\\frac {1}{\\lambda }}\\chi _{n-1}^{2}.}\n\nSampling from an inverse-Gaussian distribution\nThe following algorithm may be used.\n\nGenerate a random variate from a normal distribution with mean 0 and standard deviation equal 1\n\n  \n    \n      \n        \n          ν\n          ∼\n          N\n          (\n          0\n          ,\n          1\n          )\n          .\n        \n      \n    \n    {\\displaystyle \\displaystyle \\nu \\sim N(0,1).}\n  \n\nSquare the value\n\n  \n    \n      \n        \n          y\n          =\n          \n            ν\n            \n              2\n            \n          \n        \n      \n    \n    {\\displaystyle \\displaystyle y=\\nu ^{2}}\n  \n\nand use the relation\n\n  \n    \n      \n        x\n        =\n        μ\n        +\n        \n          \n            \n              \n                μ\n                \n                  2\n                \n              \n              y\n            \n            \n              2\n              λ\n            \n          \n        \n        −\n        \n          \n            μ\n            \n              2\n              λ\n            \n          \n        \n        \n          \n            4\n            μ\n            λ\n            y\n            +\n            \n              μ\n              \n                2\n              \n            \n            \n              y\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle x=\\mu +{\\frac {\\mu ^{2}y}{2\\lambda }}-{\\frac {\\mu }{2\\lambda }}{\\sqrt {4\\mu \\lambda y+\\mu ^{2}y^{2}}}.}\n  \n\nGenerate another random variate, this time sampled from a uniform distribution between 0 and 1\n\n  \n    \n      \n        \n          z\n          ∼\n          U\n          (\n          0\n          ,\n          1\n          )\n          .\n        \n      \n    \n    {\\displaystyle \\displaystyle z\\sim U(0,1).}\n  \n\nIf\n\n  \n    \n      \n        z\n        ≤\n        \n          \n            μ\n            \n              μ\n              +\n              x\n            \n          \n        \n      \n    \n    {\\displaystyle z\\leq {\\frac {\\mu }{\\mu +x}}}\n  \n\nthen return\n\n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\displaystyle x}\n  \n\nelse return\n\n  \n    \n      \n        \n          \n            \n              μ\n              \n                2\n              \n            \n            x\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mu ^{2}}{x}}.}\n  \n\nSample code in Java:\n\nAnd to plot Wald distribution in Python using matplotlib and NumPy:\n\nRelated distributions\nIf \n  \n    \n      \n        X\n        ∼\n        IG\n        ⁡\n        (\n        μ\n        ,\n        λ\n        )\n      \n    \n    {\\displaystyle X\\sim \\operatorname {IG} (\\mu ,\\lambda )}\n  \n, then \n  \n    \n      \n        k\n        X\n        ∼\n        IG\n        ⁡\n        (\n        k\n        μ\n        ,\n        k\n        λ\n        )\n      \n    \n    {\\displaystyle kX\\sim \\operatorname {IG} (k\\mu ,k\\lambda )}\n  \n for any number \n  \n    \n      \n        k\n        >\n        0.\n      \n    \n    {\\displaystyle k>0.}\n  \n\nIf \n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n        ∼\n        IG\n        ⁡\n        (\n        μ\n        ,\n        λ\n        )\n        \n      \n    \n    {\\displaystyle X_{i}\\sim \\operatorname {IG} (\\mu ,\\lambda )\\,}\n  \n then \n  \n    \n      \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          X\n          \n            i\n          \n        \n        ∼\n        IG\n        ⁡\n        (\n        n\n        μ\n        ,\n        \n          n\n          \n            2\n          \n        \n        λ\n        )\n        \n      \n    \n    {\\displaystyle \\sum _{i=1}^{n}X_{i}\\sim \\operatorname {IG} (n\\mu ,n^{2}\\lambda )\\,}\n  \n\nIf \n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n        ∼\n        IG\n        ⁡\n        (\n        μ\n        ,\n        λ\n        )\n        \n      \n    \n    {\\displaystyle X_{i}\\sim \\operatorname {IG} (\\mu ,\\lambda )\\,}\n  \n for \n  \n    \n      \n        i\n        =\n        1\n        ,\n        …\n        ,\n        n\n        \n      \n    \n    {\\displaystyle i=1,\\ldots ,n\\,}\n  \n then \n  \n    \n      \n        \n          \n            \n              X\n              ¯\n            \n          \n        \n        ∼\n        IG\n        ⁡\n        (\n        μ\n        ,\n        n\n        λ\n        )\n        \n      \n    \n    {\\displaystyle {\\bar {X}}\\sim \\operatorname {IG} (\\mu ,n\\lambda )\\,}\n  \n\nIf \n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n        ∼\n        IG\n        ⁡\n        (\n        \n          μ\n          \n            i\n          \n        \n        ,\n        2\n        \n          μ\n          \n            i\n          \n          \n            2\n          \n        \n        )\n        \n      \n    \n    {\\displaystyle X_{i}\\sim \\operatorname {IG} (\\mu _{i},2\\mu _{i}^{2})\\,}\n  \n then \n  \n    \n      \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          X\n          \n            i\n          \n        \n        ∼\n        IG\n        ⁡\n        \n          (\n          \n            \n              ∑\n              \n                i\n                =\n                1\n              \n              \n                n\n              \n            \n            \n              μ\n              \n                i\n              \n            \n            ,\n            2\n            \n              \n                (\n                \n                  \n                    ∑\n                    \n                      i\n                      =\n                      1\n                    \n                    \n                      n\n                    \n                  \n                  \n                    μ\n                    \n                      i\n                    \n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n          )\n        \n        \n      \n    \n    {\\displaystyle \\sum _{i=1}^{n}X_{i}\\sim \\operatorname {IG} \\left(\\sum _{i=1}^{n}\\mu _{i},2\\left(\\sum _{i=1}^{n}\\mu _{i}\\right)^{2}\\right)\\,}\n  \n\nIf \n  \n    \n      \n        X\n        ∼\n        IG\n        ⁡\n        (\n        μ\n        ,\n        λ\n        )\n      \n    \n    {\\displaystyle X\\sim \\operatorname {IG} (\\mu ,\\lambda )}\n  \n, then \n  \n    \n      \n        λ\n        (\n        X\n        −\n        μ\n        \n          )\n          \n            2\n          \n        \n        \n          /\n        \n        \n          μ\n          \n            2\n          \n        \n        X\n        ∼\n        \n          χ\n          \n            2\n          \n        \n        (\n        1\n        )\n      \n    \n    {\\displaystyle \\lambda (X-\\mu )^{2}/\\mu ^{2}X\\sim \\chi ^{2}(1)}\n  \n.\nThe convolution of an inverse Gaussian distribution (a Wald distribution) and an exponential (an ex-Wald distribution) is used as a model for response times in psychology, with visual search as one example.\n\nHistory\nThis distribution appears to have been first derived in 1900 by Louis Bachelier as the time a stock reaches a certain price for the first time. In 1915 it was used independently by Erwin Schrödinger and Marian v. Smoluchowski as the time to first passage of a Brownian motion. In the field of reproduction modeling it is known as the Hadwiger function, after Hugo Hadwiger who described it in 1940. Abraham Wald re-derived this distribution in 1944 as the limiting form of a sample in a sequential probability ratio test. The name inverse Gaussian was proposed by Maurice Tweedie in 1945. Tweedie investigated this distribution in 1956 and 1957 and established some of its statistical properties. The distribution was extensively reviewed by Folks and Chhikara in 1978.\n\nRated Inverse Gaussian Distribution\nAssuming that the time intervals between occurrences of a random phenomenon follow an inverse Gaussian distribution, the probability distribution for the number of occurrences of this event within a specified time window is referred to as rated inverse Gaussian. While, first and second moment of this distribution are calculated, the derivation of the moment generating function remains an open problem.\n\nNumeric computation and software\nDespite the simple formula for the probability density function, numerical probability calculations for the inverse Gaussian distribution nevertheless require special care to achieve full machine accuracy in floating point arithmetic for all parameter values.  Functions for the inverse Gaussian distribution are provided for the R programming language by several packages including rmutil, SuppDists, STAR, invGauss, LaplacesDemon, and statmod.\n\nSee also\nGeneralized inverse Gaussian distribution\nTweedie distributions—The inverse Gaussian distribution is a member of the family of Tweedie exponential dispersion models\nStopping time\n\nReferences\nFurther reading\nHøyland, Arnljot; Rausand, Marvin (1994). System Reliability Theory. New York: Wiley. ISBN 978-0-471-59397-3.\nSeshadri, V. (1993). The Inverse Gaussian Distribution. Oxford University Press. ISBN 978-0-19-852243-0.\n\nExternal links\nInverse Gaussian Distribution in Wolfram website.\n",
  "categories": [
    "Category:Articles contradicting other articles",
    "Category:Articles with example Java code",
    "Category:Articles with example Python (programming language) code",
    "Category:Articles with short description",
    "Category:CS1: long volume value",
    "Category:CS1 French-language sources (fr)",
    "Category:CS1 German-language sources (de)",
    "Category:Continuous distributions",
    "Category:Exponential family distributions",
    "Category:Infinitely divisible probability distributions",
    "Category:Short description matches Wikidata"
  ],
  "archived_date": "20241221_183049",
  "source_url": "https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution"
}