{
  "title": "Wang and Landau algorithm",
  "summary": "The Wang and Landau algorithm, proposed by  Fugao Wang and David P. Landau, is a Monte Carlo method designed to estimate the density of states of a system. The method performs a non-Markovian random walk to build the density of states by quickly visiting all the available energy spectrum. The Wang and Landau algorithm is an important method to obtain the density of states required to perform a multicanonical simulation.\nThe Wang–Landau algorithm can be applied to any system which is characterize",
  "content": "---\ntitle: Wang and Landau algorithm\nurl: https://en.wikipedia.org/wiki/Wang_and_Landau_algorithm\nlanguage: en\ncategories: [\"Category:Articles with example Python (programming language) code\", \"Category:Computational physics\", \"Category:Markov chain Monte Carlo\", \"Category:Statistical algorithms\"]\nreferences: 0\nlast_modified: 2024-11-28T17:07:00Z\n---\n\n# Wang and Landau algorithm\n\n## Summary\n\nThe Wang and Landau algorithm, proposed by  Fugao Wang and David P. Landau, is a Monte Carlo method designed to estimate the density of states of a system. The method performs a non-Markovian random walk to build the density of states by quickly visiting all the available energy spectrum. The Wang and Landau algorithm is an important method to obtain the density of states required to perform a multicanonical simulation.\nThe Wang–Landau algorithm can be applied to any system which is characterize\n\n## Full Content\n\nThe Wang and Landau algorithm, proposed by  Fugao Wang and David P. Landau, is a Monte Carlo method designed to estimate the density of states of a system. The method performs a non-Markovian random walk to build the density of states by quickly visiting all the available energy spectrum. The Wang and Landau algorithm is an important method to obtain the density of states required to perform a multicanonical simulation.\nThe Wang–Landau algorithm can be applied to any system which is characterized by a cost (or energy) function. For instance,\nit has been applied to the solution of numerical integrals and the folding of proteins.\nThe Wang–Landau sampling is related to the metadynamics algorithm.\n\nOverview\nThe Wang and Landau algorithm is used to obtain an estimate for the density of states of a system characterized by a cost function. It uses a non-Markovian stochastic process which asymptotically converges to a multicanonical ensemble. (I.e. to a Metropolis–Hastings algorithm with sampling distribution inverse to the density of states) The major consequence is that this sampling distribution leads to a simulation where the energy barriers are invisible. This means that the algorithm visits all the accessible states (favorable and less favorable) much faster than a Metropolis algorithm.\n\nAlgorithm\nConsider a system defined on a phase space \n  \n    \n      \n        Ω\n      \n    \n    {\\displaystyle \\Omega }\n  \n, and a cost function, E, (e.g. the energy), bounded on a spectrum \n  \n    \n      \n        E\n        ∈\n        Γ\n        =\n        [\n        \n          E\n          \n            min\n          \n        \n        ,\n        \n          E\n          \n            max\n          \n        \n        ]\n      \n    \n    {\\displaystyle E\\in \\Gamma =[E_{\\min },E_{\\max }]}\n  \n, which has an associated density of states \n  \n    \n      \n        ρ\n        (\n        E\n        )\n      \n    \n    {\\displaystyle \\rho (E)}\n  \n, which is to be estimated. The estimator is \n  \n    \n      \n        \n          \n            \n              ρ\n              ^\n            \n          \n        \n        (\n        E\n        )\n        ≡\n        exp\n        ⁡\n        (\n        S\n        (\n        E\n        )\n        )\n      \n    \n    {\\displaystyle {\\hat {\\rho }}(E)\\equiv \\exp(S(E))}\n  \n. Because Wang and Landau algorithm works in discrete spectra, the spectrum \n  \n    \n      \n        Γ\n      \n    \n    {\\displaystyle \\Gamma }\n  \n is divided in N discrete values with a difference between them of \n  \n    \n      \n        Δ\n      \n    \n    {\\displaystyle \\Delta }\n  \n, such that\n\n  \n    \n      \n        Δ\n        =\n        \n          \n            \n              \n                E\n                \n                  max\n                \n              \n              −\n              \n                E\n                \n                  min\n                \n              \n            \n            N\n          \n        \n      \n    \n    {\\displaystyle \\Delta ={\\frac {E_{\\max }-E_{\\min }}{N}}}\n  \n.\nGiven this discrete spectrum, the algorithm is initialized by:\n\nsetting all entries of the microcanonical entropy to zero, \n  \n    \n      \n        S\n        (\n        \n          E\n          \n            i\n          \n        \n        )\n        =\n        0\n         \n         \n        i\n        =\n        1\n        ,\n        2\n        ,\n        .\n        .\n        .\n        ,\n        N\n      \n    \n    {\\displaystyle S(E_{i})=0\\ \\ i=1,2,...,N}\n  \n\ninitializing \n  \n    \n      \n        f\n        =\n        1\n      \n    \n    {\\displaystyle f=1}\n  \n and\ninitializing the system randomly, by putting in a random configuration \n  \n    \n      \n        \n          r\n        \n        ∈\n        Ω\n      \n    \n    {\\displaystyle {\\boldsymbol {r}}\\in \\Omega }\n  \n.\nThe algorithm then performs a multicanonical ensemble simulation: a Metropolis–Hastings random walk in the phase space of the system with a probability distribution given by \n  \n    \n      \n        P\n        (\n        \n          r\n        \n        )\n        =\n        1\n        \n          /\n        \n        \n          \n            \n              ρ\n              ^\n            \n          \n        \n        (\n        E\n        (\n        \n          r\n        \n        )\n        )\n        =\n        exp\n        ⁡\n        (\n        −\n        S\n        (\n        E\n        (\n        \n          r\n        \n        )\n        )\n        )\n      \n    \n    {\\displaystyle P({\\boldsymbol {r}})=1/{\\hat {\\rho }}(E({\\boldsymbol {r}}))=\\exp(-S(E({\\boldsymbol {r}})))}\n  \n and a probability of proposing a new state given by a probability distribution \n  \n    \n      \n        g\n        (\n        \n          r\n        \n        →\n        \n          \n            r\n          \n          ′\n        \n        )\n      \n    \n    {\\displaystyle g({\\boldsymbol {r}}\\rightarrow {\\boldsymbol {r}}')}\n  \n. A histogram \n  \n    \n      \n        H\n        (\n        E\n        )\n      \n    \n    {\\displaystyle H(E)}\n  \n of visited energies is stored. Like in the Metropolis–Hastings algorithm, a proposal-acceptance step is performed, and consists in (see Metropolis–Hastings algorithm overview):\n\nproposing a state \n  \n    \n      \n        \n          \n            r\n          \n          ′\n        \n        ∈\n        Ω\n      \n    \n    {\\displaystyle {\\boldsymbol {r}}'\\in \\Omega }\n  \n according to the arbitrary proposal distribution \n  \n    \n      \n        g\n        (\n        \n          r\n        \n        →\n        \n          \n            r\n          \n          ′\n        \n        )\n      \n    \n    {\\displaystyle g({\\boldsymbol {r}}\\rightarrow {\\boldsymbol {r}}')}\n  \n\naccept/refuse the proposed state according to\n\n  \n    \n      \n        A\n        (\n        \n          r\n        \n        →\n        \n          \n            r\n          \n          ′\n        \n        )\n        =\n        min\n        \n          (\n          \n            1\n            ,\n            \n              e\n              \n                S\n                −\n                \n                  S\n                  ′\n                \n              \n            \n            \n              \n                \n                  g\n                  (\n                  \n                    \n                      r\n                    \n                    ′\n                  \n                  →\n                  \n                    r\n                  \n                  )\n                \n                \n                  g\n                  (\n                  \n                    r\n                  \n                  →\n                  \n                    \n                      r\n                    \n                    ′\n                  \n                  )\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle A({\\boldsymbol {r}}\\rightarrow {\\boldsymbol {r}}')=\\min \\left(1,e^{S-S'}{\\frac {g({\\boldsymbol {r}}'\\rightarrow {\\boldsymbol {r}})}{g({\\boldsymbol {r}}\\rightarrow {\\boldsymbol {r}}')}}\\right)}\n  \n\nwhere \n  \n    \n      \n        S\n        =\n        S\n        (\n        E\n        (\n        \n          r\n        \n        )\n        )\n      \n    \n    {\\displaystyle S=S(E({\\boldsymbol {r}}))}\n  \n and \n  \n    \n      \n        \n          S\n          ′\n        \n        =\n        S\n        (\n        E\n        (\n        \n          \n            r\n          \n          ′\n        \n        )\n        )\n      \n    \n    {\\displaystyle S'=S(E({\\boldsymbol {r}}'))}\n  \n.\nAfter each proposal-acceptance step, the system transits to some value \n  \n    \n      \n        \n          E\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle E_{i}}\n  \n, \n  \n    \n      \n        H\n        (\n        \n          E\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle H(E_{i})}\n  \n is incremented by one and the following update is performed:\n\n  \n    \n      \n        S\n        (\n        \n          E\n          \n            i\n          \n        \n        )\n        ←\n        S\n        (\n        \n          E\n          \n            i\n          \n        \n        )\n        +\n        f\n      \n    \n    {\\displaystyle S(E_{i})\\leftarrow S(E_{i})+f}\n  \n.\nThis is the crucial step of the algorithm, and it is what makes the Wang and Landau algorithm non-Markovian: the stochastic process now depends on the history of the process. Hence the next time there is a proposal to a state with that particular energy \n  \n    \n      \n        \n          E\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle E_{i}}\n  \n, that proposal is now more likely refused; in this sense, the algorithm forces the system to visit all of the spectrum equally. The consequence is that the histogram \n  \n    \n      \n        H\n        (\n        E\n        )\n      \n    \n    {\\displaystyle H(E)}\n  \n is more and more flat. However, this flatness depends on how well-approximated the calculated entropy is to the exact entropy, which naturally depends on the value of f. To better and better approximate the exact entropy (and thus histogram's flatness), f is decreased after M proposal-acceptance steps:\n\n  \n    \n      \n        f\n        ←\n        f\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle f\\leftarrow f/2}\n  \n.\nIt was later shown that updating the f by constantly dividing by two can lead to saturation errors. A small modification to the Wang and Landau method to avoid this problem is to use the f factor proportional to \n  \n    \n      \n        1\n        \n          /\n        \n        t\n      \n    \n    {\\displaystyle 1/t}\n  \n, where \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n is proportional to the number of steps of the simulation.\n\nTest system\nWe want to obtain the DOS for the harmonic oscillator potential.\n\n  \n    \n      \n        E\n        (\n        x\n        )\n        =\n        \n          x\n          \n            2\n          \n        \n        ,\n        \n      \n    \n    {\\displaystyle E(x)=x^{2},\\,}\n  \n\nThe analytical DOS is given by,\n\n  \n    \n      \n        ρ\n        (\n        E\n        )\n        =\n        ∫\n        δ\n        (\n        E\n        (\n        x\n        )\n        −\n        E\n        )\n        \n        d\n        x\n        =\n        ∫\n        δ\n        (\n        \n          x\n          \n            2\n          \n        \n        −\n        E\n        )\n        \n        d\n        x\n        ,\n      \n    \n    {\\displaystyle \\rho (E)=\\int \\delta (E(x)-E)\\,dx=\\int \\delta (x^{2}-E)\\,dx,}\n  \n\nby performing the last integral we obtain\n\n  \n    \n      \n        ρ\n        (\n        E\n        )\n        ∝\n        \n          \n            {\n            \n              \n                \n                  \n                    E\n                    \n                      −\n                      1\n                      \n                        /\n                      \n                      2\n                    \n                  \n                  ,\n                  \n                    for \n                  \n                  x\n                  ∈\n                  \n                    \n                      R\n                    \n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    const\n                  \n                  ,\n                  \n                    for \n                  \n                  x\n                  ∈\n                  \n                    \n                      R\n                    \n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    E\n                    \n                      1\n                      \n                        /\n                      \n                      2\n                    \n                  \n                  ,\n                  \n                    for \n                  \n                  x\n                  ∈\n                  \n                    \n                      R\n                    \n                    \n                      3\n                    \n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle \\rho (E)\\propto {\\begin{cases}E^{-1/2},{\\text{for }}x\\in \\mathbb {R} ^{1}\\\\{\\text{const}},{\\text{for }}x\\in \\mathbb {R} ^{2}\\\\E^{1/2},{\\text{for }}x\\in \\mathbb {R} ^{3}\\\\\\end{cases}}}\n  \n\nIn general, the DOS for a multidimensional harmonic oscillator will be given by some power of E, the exponent will be a function of the dimension of the system.\nHence, we can use a simple harmonic oscillator potential to test the accuracy of Wang–Landau algorithm because we know already the analytic form of the density of states. Therefore, we compare the estimated density of states \n  \n    \n      \n        \n          \n            \n              ρ\n              ^\n            \n          \n        \n        (\n        E\n        )\n      \n    \n    {\\displaystyle {\\hat {\\rho }}(E)}\n  \n obtained by the Wang–Landau algorithm with \n  \n    \n      \n        ρ\n        (\n        E\n        )\n      \n    \n    {\\displaystyle \\rho (E)}\n  \n.\n\nSample code\nThe following is a sample code of the Wang–Landau algorithm in Python, where we assume that a symmetric proposal distribution g is used:\n\n  \n    \n      \n        g\n        (\n        \n          \n            x\n          \n          ′\n        \n        →\n        \n          x\n        \n        )\n        =\n        g\n        (\n        \n          x\n        \n        →\n        \n          \n            x\n          \n          ′\n        \n        )\n      \n    \n    {\\displaystyle g({\\boldsymbol {x}}'\\rightarrow {\\boldsymbol {x}})=g({\\boldsymbol {x}}\\rightarrow {\\boldsymbol {x}}')}\n  \n\nThe code considers a \"system\" which is the underlying system being studied.\n\nWang and Landau molecular dynamics: Statistical Temperature Molecular Dynamics (STMD)\nMolecular dynamics (MD) is usually preferable to Monte Carlo (MC), so it is desirable to have a MD algorithm incorporating the basic WL idea for flat energy sampling. That algorithm is Statistical Temperature Molecular Dynamics (STMD), developed  by Jaegil Kim et al at Boston University.\nAn essential first step was made with the Statistical Temperature Monte Carlo (STMC) algorithm.  WLMC requires an extensive increase in the number of energy bins with system size, caused by working directly with the density of states. STMC is centered on an intensive quantity, the statistical temperature, \n  \n    \n      \n        T\n        (\n        E\n        )\n        =\n        1\n        \n          /\n        \n        (\n        d\n        S\n        (\n        E\n        )\n        \n          /\n        \n        d\n        E\n        )\n      \n    \n    {\\displaystyle T(E)=1/(dS(E)/dE)}\n  \n, where E is the potential energy. When combined with the relation, \n  \n    \n      \n        Ω\n        (\n        E\n        )\n        =\n        \n          e\n          \n            S\n            (\n            E\n            )\n          \n        \n      \n    \n    {\\displaystyle \\Omega (E)=e^{S(E)}}\n  \n, where we set \n  \n    \n      \n        \n          k\n          \n            B\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle k_{B}=1}\n  \n, the WL rule for updating the density of states gives  the rule for updating the discretized statistical temperature,\n\n  \n    \n      \n        \n          \n            \n              \n                T\n                ~\n              \n            \n          \n          \n            j\n            ±\n            1\n          \n          \n            ′\n          \n        \n        =\n        \n          α\n          \n            j\n            ±\n            1\n          \n        \n        \n          \n            \n              \n                T\n                ~\n              \n            \n          \n          \n            j\n            ±\n            1\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\tilde {T}}_{j\\pm 1}^{\\prime }=\\alpha _{j\\pm 1}{\\tilde {T}}_{j\\pm 1},}\n  \n\nwhere \n  \n    \n      \n        \n          α\n          \n            j\n            ±\n            1\n          \n        \n        =\n        1\n        \n          /\n        \n        (\n        1\n        ∓\n        δ\n        f\n        \n          \n            \n              \n                T\n                ~\n              \n            \n          \n          \n            j\n            ±\n            1\n          \n        \n        )\n        ,\n        δ\n        f\n        =\n        (\n        l\n        n\n        f\n        \n          /\n        \n        2\n        Δ\n        E\n        )\n        ,\n        Δ\n        E\n      \n    \n    {\\displaystyle \\alpha _{j\\pm 1}=1/(1\\mp \\delta f{\\tilde {T}}_{j\\pm 1}),\\delta f=(lnf/2\\Delta E),\\Delta E}\n  \n is the energy bin size, and \n  \n    \n      \n        \n          \n            \n              T\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tilde {T}}}\n  \n denotes the running estimate. We define f as in, a factor >1 that multiplies the estimate of the DOS for the i'th energy bin when the system visits an energy in that bin.\nThe details are given in Ref. With an initial guess for \n  \n    \n      \n        T\n        (\n        E\n        )\n      \n    \n    {\\displaystyle T(E)}\n  \n and the range restricted to lie between \n  \n    \n      \n        \n          T\n          \n            L\n          \n        \n      \n    \n    {\\displaystyle T_{L}}\n  \n and \n  \n    \n      \n        \n          T\n          \n            U\n          \n        \n      \n    \n    {\\displaystyle T_{U}}\n  \n, the simulation proceeds as in WLMC, with significant numerical differences. An interpolation of \n  \n    \n      \n        \n          \n            \n              T\n              ~\n            \n          \n        \n        (\n        E\n        )\n      \n    \n    {\\displaystyle {\\tilde {T}}(E)}\n  \n gives a continuum expression of the estimated \n  \n    \n      \n        S\n        (\n        E\n        )\n      \n    \n    {\\displaystyle S(E)}\n  \n upon integration of its inverse, allowing the use of larger energy bins than in WL. Different values of \n  \n    \n      \n        S\n        (\n        E\n        )\n      \n    \n    {\\displaystyle S(E)}\n  \n are available within the same energy bin when evaluating the acceptance probability. When histogram fluctuations are less than 20% of the mean, \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n  is reduced according to \n  \n    \n      \n        f\n        →\n        \n          \n            f\n          \n        \n      \n    \n    {\\displaystyle f\\rightarrow {\\sqrt {f}}}\n  \n.\nSTMC was compared with WL for the Ising model and the Lennard-Jones liquid. Upon increasing energy bin size, STMC gets the same results over a considerable range, while the performance of WL deteriorates rapidly. STMD can use smaller initial values of \n  \n    \n      \n        \n          f\n          \n            d\n          \n        \n        =\n        f\n        −\n        1\n      \n    \n    {\\displaystyle f_{d}=f-1}\n  \n for more rapid convergence. In sum, STMC needs fewer steps to obtain the same quality of results.\nNow consider the main result, STMD. It is based on the observation that in a standard MD simulation at temperature \n  \n    \n      \n        \n          T\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle T_{0}}\n  \n with forces derived from the potential energy \n  \n    \n      \n        E\n        (\n        [\n        x\n        ]\n        )\n      \n    \n    {\\displaystyle E([x])}\n  \n, where \n  \n    \n      \n        [\n        x\n        ]\n      \n    \n    {\\displaystyle [x]}\n  \n denotes all the positions, the sampling weight for a configuration is \n  \n    \n      \n        \n          e\n          \n            −\n            E\n            (\n            [\n            x\n            ]\n            )\n            \n              /\n            \n            \n              T\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle e^{-E([x])/T_{0}}}\n  \n. Furthermore, if the forces are derived from a function \n  \n    \n      \n        W\n        (\n        E\n        )\n      \n    \n    {\\displaystyle W(E)}\n  \n, the sampling weight is \n  \n    \n      \n        \n          e\n          \n            −\n            W\n            (\n            E\n            (\n            [\n            x\n            ]\n            )\n            )\n            \n              /\n            \n            \n              T\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle e^{-W(E([x]))/T_{0}}}\n  \n.\nFor flat energy sampling, let the effective potential be \n  \n    \n      \n        \n          T\n          \n            0\n          \n        \n        S\n        (\n        E\n        )\n      \n    \n    {\\displaystyle T_{0}S(E)}\n  \n - entropic molecular dynamics. Then the weight is \n  \n    \n      \n        \n          e\n          \n            −\n            S\n            (\n            E\n            )\n          \n        \n      \n    \n    {\\displaystyle e^{-S(E)}}\n  \n. Since the density of states is \n  \n    \n      \n        \n          e\n          \n            +\n            S\n            (\n            E\n            )\n          \n        \n      \n    \n    {\\displaystyle e^{+S(E)}}\n  \n, their product gives flat energy sampling.\nThe forces are calculated as \n\n  \n    \n      \n        F\n        =\n        (\n        −\n        d\n        \n          /\n        \n        d\n        x\n        )\n        \n          T\n          \n            0\n          \n        \n        S\n        (\n        E\n        )\n        =\n        \n          T\n          \n            0\n          \n        \n        (\n        d\n        S\n        \n          /\n        \n        d\n        E\n        )\n        (\n        −\n        d\n        \n          /\n        \n        d\n        x\n        )\n        E\n        (\n        [\n        x\n        ]\n        )\n        =\n        (\n        \n          T\n          \n            0\n          \n        \n        \n          /\n        \n        T\n        (\n        E\n        )\n        )\n        \n          F\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle F=(-d/dx)T_{0}S(E)=T_{0}(dS/dE)(-d/dx)E([x])=(T_{0}/T(E))F^{0}}\n  \n\nwhere \n  \n    \n      \n        \n          F\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle F^{0}}\n  \n denotes the usual force derived from the potential energy. Scaling the usual forces by the factor \n  \n    \n      \n        (\n        \n          T\n          \n            0\n          \n        \n        \n          /\n        \n        T\n        (\n        E\n        )\n        )\n      \n    \n    {\\displaystyle (T_{0}/T(E))}\n  \n produces flat energy sampling. \nSTMD starts with an ordinary MD algorithm at constant \n  \n    \n      \n        \n          T\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle T_{0}}\n  \n and V. The forces are scaled as indicated, and the statistical temperature is updated every time step, using the same procedure as in STMC. As the simulation converges to flat energy sampling, the running estimate \n  \n    \n      \n        \n          \n            \n              T\n              ~\n            \n          \n        \n        (\n        E\n        )\n      \n    \n    {\\displaystyle {\\tilde {T}}(E)}\n  \n converges to the true \n  \n    \n      \n        T\n        (\n        E\n        )\n      \n    \n    {\\displaystyle T(E)}\n  \n. Technical details including steps to speed convergence are described in  and.\nIn STMD \n  \n    \n      \n        \n          T\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle T_{0}}\n  \n is called the kinetic temperature as it controls the velocities as usual, but does not enter the configurational sampling, which is unusual. Thus STMD can probe low energies with fast particles. Any canonical average can be calculated with reweighting, but the statistical temperature, \n  \n    \n      \n        T\n        (\n        E\n        )\n      \n    \n    {\\displaystyle T(E)}\n  \n, is immediately available with no additional analysis. It is extremely valuable for studying phase transitions. In finite nanosystems \n  \n    \n      \n        T\n        (\n        E\n        )\n      \n    \n    {\\displaystyle T(E)}\n  \n has a feature corresponding to every “subphase transition”. For a sufficiently strong transition, an equal-area construction on an S-loop in \n  \n    \n      \n        1\n        \n          /\n        \n        T\n        (\n        E\n        )\n      \n    \n    {\\displaystyle 1/T(E)}\n  \n gives the transition temperature.\nSTMD has been refined by the BU group, and applied to several systems by them and others.  It was recognized by D. Stelter that despite our emphasis on working with intensive quantities, \n  \n    \n      \n        l\n        n\n        (\n        f\n        )\n      \n    \n    {\\displaystyle ln(f)}\n  \n is extensive. However \n  \n    \n      \n        δ\n        f\n        =\n        (\n        l\n        n\n        (\n        f\n        )\n        \n          /\n        \n        2\n        Δ\n        E\n        )\n      \n    \n    {\\displaystyle \\delta f=(ln(f)/2\\Delta E)}\n  \n is intensive, and the procedure \n  \n    \n      \n        f\n        →\n        \n          \n            f\n          \n        \n      \n    \n    {\\displaystyle f\\rightarrow {\\sqrt {f}}}\n  \n based on histogram flatness is  replaced by cutting \n  \n    \n      \n        δ\n        f\n      \n    \n    {\\displaystyle \\delta f}\n  \n in half every fixed number of time steps. This simple change makes STMD entirely intensive and substantially improves performance for large systems. Furthermore, the final value of the intensive \n  \n    \n      \n        δ\n        f\n      \n    \n    {\\displaystyle \\delta f}\n  \n is a constant that determines the magnitude of error in the converged \n  \n    \n      \n        T\n        (\n        E\n        )\n      \n    \n    {\\displaystyle T(E)}\n  \n, and is independent of system size. STMD is implemented in LAMMPS as fix stmd.\nSTMD is particularly useful for phase transitions. Equilibrium information is impossible to obtain with a canonical simulation, as supercooling or superheating is necessary to cause the transition. However an STMD run obtains flat energy sampling with a natural progression of heating and cooling, without getting trapped in the low energy or high energy state. Most recently it has been applied to the fluid/gel transition  in lipid-wrapped nanoparticles.\nReplica exchange STMD  has also been presented by the BU group.\n\n\n== References ==\n",
  "categories": [
    "Category:Articles with example Python (programming language) code",
    "Category:Computational physics",
    "Category:Markov chain Monte Carlo",
    "Category:Statistical algorithms"
  ],
  "archived_date": "20241220_214755",
  "source_url": "https://en.wikipedia.org/wiki/Wang_and_Landau_algorithm"
}