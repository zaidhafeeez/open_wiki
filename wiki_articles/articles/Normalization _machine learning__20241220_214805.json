{
  "title": "Normalization (machine learning)",
  "summary": "In machine learning, normalization is a statistical technique with various applications. There are two main forms of normalization, namely data normalization and activation normalization. Data normalization (or feature scaling) includes methods that rescale input data so that the features have the same range, mean, variance, or other statistical properties. For instance, a popular choice of feature scaling method is min-max normalization, where each feature is transformed to have the same range ",
  "content": "---\ntitle: Normalization (machine learning)\nurl: https://en.wikipedia.org/wiki/Normalization_(machine_learning)\nlanguage: en\ncategories: [\"Category:Articles with example Python (programming language) code\", \"Category:Articles with short description\", \"Category:CS1 errors: missing periodical\", \"Category:Deep learning\", \"Category:Machine learning\", \"Category:Neural networks\", \"Category:Short description matches Wikidata\", \"Category:Statistical data transformation\"]\nreferences: 0\nlast_modified: 2024-12-20T07:25:30Z\n---\n\n# Normalization (machine learning)\n\n## Summary\n\nIn machine learning, normalization is a statistical technique with various applications. There are two main forms of normalization, namely data normalization and activation normalization. Data normalization (or feature scaling) includes methods that rescale input data so that the features have the same range, mean, variance, or other statistical properties. For instance, a popular choice of feature scaling method is min-max normalization, where each feature is transformed to have the same range \n\n## Full Content\n\nIn machine learning, normalization is a statistical technique with various applications. There are two main forms of normalization, namely data normalization and activation normalization. Data normalization (or feature scaling) includes methods that rescale input data so that the features have the same range, mean, variance, or other statistical properties. For instance, a popular choice of feature scaling method is min-max normalization, where each feature is transformed to have the same range (typically \n  \n    \n      \n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle [0,1]}\n  \n or \n  \n    \n      \n        [\n        −\n        1\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle [-1,1]}\n  \n). This solves the problem of different features having vastly different scales, for example if one feature is measured in kilometers and another in nanometers.\nActivation normalization, on the other hand, is specific to deep learning, and includes methods that rescale the activation of hidden neurons inside neural networks.\nNormalization is often used to:\n\nincrease the speed of training convergence,\nreduce sensitivity to variations and feature scales in input data,\nreduce overfitting,\nand produce better model generalization to unseen data.\nNormalization techniques are often theoretically justified as reducing covariance shift, smoothing optimization landscapes, and increasing regularization, though they are mainly justified by empirical success.\n\nBatch normalization\nBatch normalization (BatchNorm) operates on the activations of a layer for each mini-batch.\nConsider a simple feedforward network, defined by chaining together modules:\n\n  \n    \n      \n        \n          x\n          \n            (\n            0\n            )\n          \n        \n        ↦\n        \n          x\n          \n            (\n            1\n            )\n          \n        \n        ↦\n        \n          x\n          \n            (\n            2\n            )\n          \n        \n        ↦\n        ⋯\n      \n    \n    {\\displaystyle x^{(0)}\\mapsto x^{(1)}\\mapsto x^{(2)}\\mapsto \\cdots }\n  \n\nwhere each network module can be a linear transform, a nonlinear activation function, a convolution, etc. \n  \n    \n      \n        \n          x\n          \n            (\n            0\n            )\n          \n        \n      \n    \n    {\\displaystyle x^{(0)}}\n  \n is the input vector, \n  \n    \n      \n        \n          x\n          \n            (\n            1\n            )\n          \n        \n      \n    \n    {\\displaystyle x^{(1)}}\n  \n is the output vector from the first module, etc.\nBatchNorm is a module that can be inserted at any point in the feedforward network. For example, suppose it is inserted just after \n  \n    \n      \n        \n          x\n          \n            (\n            l\n            )\n          \n        \n      \n    \n    {\\displaystyle x^{(l)}}\n  \n, then the network would operate accordingly:\n\n  \n    \n      \n        ⋯\n        ↦\n        \n          x\n          \n            (\n            l\n            )\n          \n        \n        ↦\n        \n          B\n          N\n        \n        (\n        \n          x\n          \n            (\n            l\n            )\n          \n        \n        )\n        ↦\n        \n          x\n          \n            (\n            l\n            +\n            1\n            )\n          \n        \n        ↦\n        ⋯\n      \n    \n    {\\displaystyle \\cdots \\mapsto x^{(l)}\\mapsto \\mathrm {BN} (x^{(l)})\\mapsto x^{(l+1)}\\mapsto \\cdots }\n  \n\nThe BatchNorm module does not operate over individual inputs. Instead, it must operate over one batch of inputs at a time.\nConcretely, suppose we have a batch of inputs \n  \n    \n      \n        \n          x\n          \n            (\n            1\n            )\n          \n          \n            (\n            0\n            )\n          \n        \n        ,\n        \n          x\n          \n            (\n            2\n            )\n          \n          \n            (\n            0\n            )\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            (\n            B\n            )\n          \n          \n            (\n            0\n            )\n          \n        \n      \n    \n    {\\displaystyle x_{(1)}^{(0)},x_{(2)}^{(0)},\\dots ,x_{(B)}^{(0)}}\n  \n, fed all at once into the network. We would obtain in the middle of the network some vectors:\n\n  \n    \n      \n        \n          x\n          \n            (\n            1\n            )\n          \n          \n            (\n            l\n            )\n          \n        \n        ,\n        \n          x\n          \n            (\n            2\n            )\n          \n          \n            (\n            l\n            )\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            (\n            B\n            )\n          \n          \n            (\n            l\n            )\n          \n        \n      \n    \n    {\\displaystyle x_{(1)}^{(l)},x_{(2)}^{(l)},\\dots ,x_{(B)}^{(l)}}\n  \n\nThe BatchNorm module computes the coordinate-wise mean and variance of these vectors:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  μ\n                  \n                    i\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    B\n                  \n                \n                \n                  ∑\n                  \n                    b\n                    =\n                    1\n                  \n                  \n                    B\n                  \n                \n                \n                  x\n                  \n                    (\n                    b\n                    )\n                    ,\n                    i\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n              \n            \n            \n              \n                (\n                \n                  σ\n                  \n                    i\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    B\n                  \n                \n                \n                  ∑\n                  \n                    b\n                    =\n                    1\n                  \n                  \n                    B\n                  \n                \n                (\n                \n                  x\n                  \n                    (\n                    b\n                    )\n                    ,\n                    i\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n                −\n                \n                  μ\n                  \n                    i\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\mu _{i}^{(l)}&={\\frac {1}{B}}\\sum _{b=1}^{B}x_{(b),i}^{(l)}\\\\(\\sigma _{i}^{(l)})^{2}&={\\frac {1}{B}}\\sum _{b=1}^{B}(x_{(b),i}^{(l)}-\\mu _{i}^{(l)})^{2}\\end{aligned}}}\n  \n\nwhere \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n indexes the coordinates of the vectors, and \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n indexes the elements of the batch. In other words, we are considering the \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n-th coordinate of each vector in the batch, and computing the mean and variance of these numbers.\nIt then normalizes each coordinate to have zero mean and unit variance:\n\n  \n    \n      \n        \n          \n            \n              \n                x\n                ^\n              \n            \n          \n          \n            (\n            b\n            )\n            ,\n            i\n          \n          \n            (\n            l\n            )\n          \n        \n        =\n        \n          \n            \n              \n                x\n                \n                  (\n                  b\n                  )\n                  ,\n                  i\n                \n                \n                  (\n                  l\n                  )\n                \n              \n              −\n              \n                μ\n                \n                  i\n                \n                \n                  (\n                  l\n                  )\n                \n              \n            \n            \n              (\n              \n                σ\n                \n                  i\n                \n                \n                  (\n                  l\n                  )\n                \n              \n              \n                )\n                \n                  2\n                \n              \n              +\n              ϵ\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {x}}_{(b),i}^{(l)}={\\frac {x_{(b),i}^{(l)}-\\mu _{i}^{(l)}}{\\sqrt {(\\sigma _{i}^{(l)})^{2}+\\epsilon }}}}\n  \n\nThe \n  \n    \n      \n        ϵ\n      \n    \n    {\\displaystyle \\epsilon }\n  \n is a small positive constant such as \n  \n    \n      \n        \n          10\n          \n            −\n            9\n          \n        \n      \n    \n    {\\displaystyle 10^{-9}}\n  \n added to the variance for numerical stability, to avoid division by zero.\nFinally, it applies a linear transformation:\n\n  \n    \n      \n        \n          y\n          \n            (\n            b\n            )\n            ,\n            i\n          \n          \n            (\n            l\n            )\n          \n        \n        =\n        \n          γ\n          \n            i\n          \n        \n        \n          \n            \n              \n                x\n                ^\n              \n            \n          \n          \n            (\n            b\n            )\n            ,\n            i\n          \n          \n            (\n            l\n            )\n          \n        \n        +\n        \n          β\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{(b),i}^{(l)}=\\gamma _{i}{\\hat {x}}_{(b),i}^{(l)}+\\beta _{i}}\n  \n\nHere, \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n and \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n are parameters inside the BatchNorm module. They are learnable parameters, typically trained by gradient descent.\nThe following is a Python implementation of BatchNorm:\n\nInterpretation\nγ\n      \n    \n    {\\displaystyle \\gamma }\n  \n and \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n allow the network to learn to undo the normalization, if this is beneficial. BatchNorm can be interpreted as removing the purely linear transformations, so that its layers focus solely on modelling the nonlinear aspects of data, which may be beneficial, as a neural network can always be augmented with a linear transformation layer on top.\nIt is claimed in the original publication that BatchNorm works by reducing internal covariance shift, though the claim has both supporters and detractors.\n\nSpecial cases\nThe original paper recommended to only use BatchNorms after a linear transform, not after a nonlinear activation. That is, \n  \n    \n      \n        ϕ\n        (\n        \n          B\n          N\n        \n        (\n        W\n        x\n        +\n        b\n        )\n        )\n      \n    \n    {\\displaystyle \\phi (\\mathrm {BN} (Wx+b))}\n  \n, not \n  \n    \n      \n        \n          B\n          N\n        \n        (\n        ϕ\n        (\n        W\n        x\n        +\n        b\n        )\n        )\n      \n    \n    {\\displaystyle \\mathrm {BN} (\\phi (Wx+b))}\n  \n. Also, the bias \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n does not matter, since it would be canceled by the subsequent mean subtraction, so it is of the form \n  \n    \n      \n        \n          B\n          N\n        \n        (\n        W\n        x\n        )\n      \n    \n    {\\displaystyle \\mathrm {BN} (Wx)}\n  \n. That is, if a BatchNorm is preceded by a linear transform, then that linear transform's bias term is set to zero.\nFor convolutional neural networks (CNNs), BatchNorm must preserve the translation-invariance of these models, meaning that it must treat all outputs of the same kernel as if they are different data points within a batch. This is sometimes called Spatial BatchNorm, or BatchNorm2D, or per-channel BatchNorm.\nConcretely, suppose we have a 2-dimensional convolutional layer defined by:\n\n  \n    \n      \n        \n          x\n          \n            h\n            ,\n            w\n            ,\n            c\n          \n          \n            (\n            l\n            )\n          \n        \n        =\n        \n          ∑\n          \n            \n              h\n              ′\n            \n            ,\n            \n              w\n              ′\n            \n            ,\n            \n              c\n              ′\n            \n          \n        \n        \n          K\n          \n            \n              h\n              ′\n            \n            −\n            h\n            ,\n            \n              w\n              ′\n            \n            −\n            w\n            ,\n            c\n            ,\n            \n              c\n              ′\n            \n          \n          \n            (\n            l\n            )\n          \n        \n        \n          x\n          \n            \n              h\n              ′\n            \n            ,\n            \n              w\n              ′\n            \n            ,\n            \n              c\n              ′\n            \n          \n          \n            (\n            l\n            −\n            1\n            )\n          \n        \n        +\n        \n          b\n          \n            c\n          \n          \n            (\n            l\n            )\n          \n        \n      \n    \n    {\\displaystyle x_{h,w,c}^{(l)}=\\sum _{h',w',c'}K_{h'-h,w'-w,c,c'}^{(l)}x_{h',w',c'}^{(l-1)}+b_{c}^{(l)}}\n  \n\nwhere:\n\n  \n    \n      \n        \n          x\n          \n            h\n            ,\n            w\n            ,\n            c\n          \n          \n            (\n            l\n            )\n          \n        \n      \n    \n    {\\displaystyle x_{h,w,c}^{(l)}}\n  \n is the activation of the neuron at position \n  \n    \n      \n        (\n        h\n        ,\n        w\n        )\n      \n    \n    {\\displaystyle (h,w)}\n  \n in the \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n-th channel of the \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  \n-th layer.\n\n  \n    \n      \n        \n          K\n          \n            Δ\n            h\n            ,\n            Δ\n            w\n            ,\n            c\n            ,\n            \n              c\n              ′\n            \n          \n          \n            (\n            l\n            )\n          \n        \n      \n    \n    {\\displaystyle K_{\\Delta h,\\Delta w,c,c'}^{(l)}}\n  \n is a kernel tensor. Each channel \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n corresponds to a kernel \n  \n    \n      \n        \n          K\n          \n            \n              h\n              ′\n            \n            −\n            h\n            ,\n            \n              w\n              ′\n            \n            −\n            w\n            ,\n            c\n            ,\n            \n              c\n              ′\n            \n          \n          \n            (\n            l\n            )\n          \n        \n      \n    \n    {\\displaystyle K_{h'-h,w'-w,c,c'}^{(l)}}\n  \n, with indices \n  \n    \n      \n        Δ\n        h\n        ,\n        Δ\n        w\n        ,\n        \n          c\n          ′\n        \n      \n    \n    {\\displaystyle \\Delta h,\\Delta w,c'}\n  \n.\n\n  \n    \n      \n        \n          b\n          \n            c\n          \n          \n            (\n            l\n            )\n          \n        \n      \n    \n    {\\displaystyle b_{c}^{(l)}}\n  \n is the bias term for the \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n-th channel of the \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  \n-th layer.\nIn order to preserve the translational invariance, BatchNorm treats all outputs from the same kernel in the same batch as more data in a batch. That is, it is applied once per kernel \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n (equivalently, once per channel \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n), not per activation \n  \n    \n      \n        \n          x\n          \n            h\n            ,\n            w\n            ,\n            c\n          \n          \n            (\n            l\n            +\n            1\n            )\n          \n        \n      \n    \n    {\\displaystyle x_{h,w,c}^{(l+1)}}\n  \n:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  μ\n                  \n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    \n                      B\n                      H\n                      W\n                    \n                  \n                \n                \n                  ∑\n                  \n                    b\n                    =\n                    1\n                  \n                  \n                    B\n                  \n                \n                \n                  ∑\n                  \n                    h\n                    =\n                    1\n                  \n                  \n                    H\n                  \n                \n                \n                  ∑\n                  \n                    w\n                    =\n                    1\n                  \n                  \n                    W\n                  \n                \n                \n                  x\n                  \n                    (\n                    b\n                    )\n                    ,\n                    h\n                    ,\n                    w\n                    ,\n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n              \n            \n            \n              \n                (\n                \n                  σ\n                  \n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    \n                      B\n                      H\n                      W\n                    \n                  \n                \n                \n                  ∑\n                  \n                    b\n                    =\n                    1\n                  \n                  \n                    B\n                  \n                \n                \n                  ∑\n                  \n                    h\n                    =\n                    1\n                  \n                  \n                    H\n                  \n                \n                \n                  ∑\n                  \n                    w\n                    =\n                    1\n                  \n                  \n                    W\n                  \n                \n                (\n                \n                  x\n                  \n                    (\n                    b\n                    )\n                    ,\n                    h\n                    ,\n                    w\n                    ,\n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n                −\n                \n                  μ\n                  \n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\mu _{c}^{(l)}&={\\frac {1}{BHW}}\\sum _{b=1}^{B}\\sum _{h=1}^{H}\\sum _{w=1}^{W}x_{(b),h,w,c}^{(l)}\\\\(\\sigma _{c}^{(l)})^{2}&={\\frac {1}{BHW}}\\sum _{b=1}^{B}\\sum _{h=1}^{H}\\sum _{w=1}^{W}(x_{(b),h,w,c}^{(l)}-\\mu _{c}^{(l)})^{2}\\end{aligned}}}\n  \n\nwhere \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is the batch size, \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n is the height of the feature map, and \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n is the width of the feature map.\nThat is, even though there are only \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n data points in a batch, all \n  \n    \n      \n        B\n        H\n        W\n      \n    \n    {\\displaystyle BHW}\n  \n outputs from the kernel in this batch are treated equally.\nSubsequently, normalization and the linear transform is also done per kernel:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    \n                      \n                        x\n                        ^\n                      \n                    \n                  \n                  \n                    (\n                    b\n                    )\n                    ,\n                    h\n                    ,\n                    w\n                    ,\n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        x\n                        \n                          (\n                          b\n                          )\n                          ,\n                          h\n                          ,\n                          w\n                          ,\n                          c\n                        \n                        \n                          (\n                          l\n                          )\n                        \n                      \n                      −\n                      \n                        μ\n                        \n                          c\n                        \n                        \n                          (\n                          l\n                          )\n                        \n                      \n                    \n                    \n                      (\n                      \n                        σ\n                        \n                          c\n                        \n                        \n                          (\n                          l\n                          )\n                        \n                      \n                      \n                        )\n                        \n                          2\n                        \n                      \n                      +\n                      ϵ\n                    \n                  \n                \n              \n            \n            \n              \n                \n                  y\n                  \n                    (\n                    b\n                    )\n                    ,\n                    h\n                    ,\n                    w\n                    ,\n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n              \n              \n                \n                =\n                \n                  γ\n                  \n                    c\n                  \n                \n                \n                  \n                    \n                      \n                        x\n                        ^\n                      \n                    \n                  \n                  \n                    (\n                    b\n                    )\n                    ,\n                    h\n                    ,\n                    w\n                    ,\n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n                +\n                \n                  β\n                  \n                    c\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\hat {x}}_{(b),h,w,c}^{(l)}&={\\frac {x_{(b),h,w,c}^{(l)}-\\mu _{c}^{(l)}}{\\sqrt {(\\sigma _{c}^{(l)})^{2}+\\epsilon }}}\\\\y_{(b),h,w,c}^{(l)}&=\\gamma _{c}{\\hat {x}}_{(b),h,w,c}^{(l)}+\\beta _{c}\\end{aligned}}}\n  \n\nSimilar considerations apply for BatchNorm for n-dimensional convolutions.\nThe following is a Python implementation of BatchNorm for 2D convolutions:\n\nImprovements\nBatchNorm has been very popular and there were many attempted improvements. Some examples include:\n\nghost batching: randomly partition a batch into sub-batches and perform BatchNorm separately on each;\nweight decay on \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n and \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n;\nand combining BatchNorm with GroupNorm.\nA particular problem with BatchNorm is that during training, the mean and variance are calculated on the fly for each batch (usually as an exponential moving average), but during inference, the mean and variance were frozen from those calculated during training. This train-test disparity degrades performance. The disparity can be decreased by simulating the moving average during inference:: Eq. 3 \n\n  \n    \n      \n        \n          \n            \n              \n                μ\n              \n              \n                \n                =\n                α\n                E\n                [\n                x\n                ]\n                +\n                (\n                1\n                −\n                α\n                )\n                \n                  μ\n                  \n                    x\n                    ,\n                    \n                       train\n                    \n                  \n                \n              \n            \n            \n              \n                \n                  σ\n                  \n                    2\n                  \n                \n              \n              \n                \n                =\n                (\n                α\n                E\n                [\n                x\n                \n                  ]\n                  \n                    2\n                  \n                \n                +\n                (\n                1\n                −\n                α\n                )\n                \n                  μ\n                  \n                    \n                      x\n                      \n                        2\n                      \n                    \n                    ,\n                    \n                       train\n                    \n                  \n                \n                )\n                −\n                \n                  μ\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\mu &=\\alpha E[x]+(1-\\alpha )\\mu _{x,{\\text{ train}}}\\\\\\sigma ^{2}&=(\\alpha E[x]^{2}+(1-\\alpha )\\mu _{x^{2},{\\text{ train}}})-\\mu ^{2}\\end{aligned}}}\n  \n\nwhere \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n is a hyperparameter to be optimized on a validation set.\nOther works attempt to eliminate BatchNorm, such as the Normalizer-Free ResNet.\n\nLayer normalization\nLayer normalization (LayerNorm) is a popular alternative to BatchNorm. Unlike BatchNorm, which normalizes activations across the batch dimension for a given feature, LayerNorm normalizes across all the features within a single data sample. Compared to BatchNorm, LayerNorm's performance is not affected by batch size. It is a key component of transformer models.\nFor a given data input and layer, LayerNorm computes the mean \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n and variance \n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}}\n  \n over all the neurons in the layer. Similar to BatchNorm, learnable parameters \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n (scale) and \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n (shift) are applied. It is defined by:\n\n  \n    \n      \n        \n          \n            \n              \n                x\n                \n                  i\n                \n              \n              ^\n            \n          \n        \n        =\n        \n          \n            \n              \n                x\n                \n                  i\n                \n              \n              −\n              μ\n            \n            \n              \n                σ\n                \n                  2\n                \n              \n              +\n              ϵ\n            \n          \n        \n        ,\n        \n        \n          y\n          \n            i\n          \n        \n        =\n        \n          γ\n          \n            i\n          \n        \n        \n          \n            \n              \n                x\n                \n                  i\n                \n              \n              ^\n            \n          \n        \n        +\n        \n          β\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\hat {x_{i}}}={\\frac {x_{i}-\\mu }{\\sqrt {\\sigma ^{2}+\\epsilon }}},\\quad y_{i}=\\gamma _{i}{\\hat {x_{i}}}+\\beta _{i}}\n  \n\nwhere:\n\n  \n    \n      \n        μ\n        =\n        \n          \n            1\n            D\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            D\n          \n        \n        \n          x\n          \n            i\n          \n        \n        ,\n        \n        \n          σ\n          \n            2\n          \n        \n        =\n        \n          \n            1\n            D\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            D\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        −\n        μ\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mu ={\\frac {1}{D}}\\sum _{i=1}^{D}x_{i},\\quad \\sigma ^{2}={\\frac {1}{D}}\\sum _{i=1}^{D}(x_{i}-\\mu )^{2}}\n  \n\nand the index \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n ranges over the neurons in that layer.\n\nExamples\nFor example, in CNN, a LayerNorm applies to all activations in a layer. In the previous notation, we have:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  μ\n                  \n                    (\n                    l\n                    )\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    \n                      H\n                      W\n                      C\n                    \n                  \n                \n                \n                  ∑\n                  \n                    h\n                    =\n                    1\n                  \n                  \n                    H\n                  \n                \n                \n                  ∑\n                  \n                    w\n                    =\n                    1\n                  \n                  \n                    W\n                  \n                \n                \n                  ∑\n                  \n                    c\n                    =\n                    1\n                  \n                  \n                    C\n                  \n                \n                \n                  x\n                  \n                    h\n                    ,\n                    w\n                    ,\n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n              \n            \n            \n              \n                (\n                \n                  σ\n                  \n                    (\n                    l\n                    )\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    \n                      H\n                      W\n                      C\n                    \n                  \n                \n                \n                  ∑\n                  \n                    h\n                    =\n                    1\n                  \n                  \n                    H\n                  \n                \n                \n                  ∑\n                  \n                    w\n                    =\n                    1\n                  \n                  \n                    W\n                  \n                \n                \n                  ∑\n                  \n                    c\n                    =\n                    1\n                  \n                  \n                    C\n                  \n                \n                (\n                \n                  x\n                  \n                    h\n                    ,\n                    w\n                    ,\n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n                −\n                \n                  μ\n                  \n                    (\n                    l\n                    )\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        x\n                        ^\n                      \n                    \n                  \n                  \n                    h\n                    ,\n                    w\n                    ,\n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        \n                          \n                            \n                              x\n                              ^\n                            \n                          \n                        \n                        \n                          h\n                          ,\n                          w\n                          ,\n                          c\n                        \n                        \n                          (\n                          l\n                          )\n                        \n                      \n                      −\n                      \n                        μ\n                        \n                          (\n                          l\n                          )\n                        \n                      \n                    \n                    \n                      (\n                      \n                        σ\n                        \n                          (\n                          l\n                          )\n                        \n                      \n                      \n                        )\n                        \n                          2\n                        \n                      \n                      +\n                      ϵ\n                    \n                  \n                \n              \n            \n            \n              \n                \n                  y\n                  \n                    h\n                    ,\n                    w\n                    ,\n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n              \n              \n                \n                =\n                \n                  γ\n                  \n                    (\n                    l\n                    )\n                  \n                \n                \n                  \n                    \n                      \n                        x\n                        ^\n                      \n                    \n                  \n                  \n                    h\n                    ,\n                    w\n                    ,\n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n                +\n                \n                  β\n                  \n                    (\n                    l\n                    )\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\mu ^{(l)}&={\\frac {1}{HWC}}\\sum _{h=1}^{H}\\sum _{w=1}^{W}\\sum _{c=1}^{C}x_{h,w,c}^{(l)}\\\\(\\sigma ^{(l)})^{2}&={\\frac {1}{HWC}}\\sum _{h=1}^{H}\\sum _{w=1}^{W}\\sum _{c=1}^{C}(x_{h,w,c}^{(l)}-\\mu ^{(l)})^{2}\\\\{\\hat {x}}_{h,w,c}^{(l)}&={\\frac {{\\hat {x}}_{h,w,c}^{(l)}-\\mu ^{(l)}}{\\sqrt {(\\sigma ^{(l)})^{2}+\\epsilon }}}\\\\y_{h,w,c}^{(l)}&=\\gamma ^{(l)}{\\hat {x}}_{h,w,c}^{(l)}+\\beta ^{(l)}\\end{aligned}}}\n  \n\nNotice that the batch index \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n is removed, while the channel index \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n is added.\nIn recurrent neural networks and transformers, LayerNorm is applied individually to each timestep. For example, if the hidden vector in an RNN at timestep \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n is \n  \n    \n      \n        \n          x\n          \n            (\n            t\n            )\n          \n        \n        ∈\n        \n          \n            R\n          \n          \n            D\n          \n        \n      \n    \n    {\\displaystyle x^{(t)}\\in \\mathbb {R} ^{D}}\n  \n, where \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n is the dimension of the hidden vector, then LayerNorm will be applied with:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  x\n                  \n                    i\n                  \n                \n                ^\n              \n            \n          \n          \n            (\n            t\n            )\n          \n        \n        =\n        \n          \n            \n              \n                x\n                \n                  i\n                \n                \n                  (\n                  t\n                  )\n                \n              \n              −\n              \n                μ\n                \n                  (\n                  t\n                  )\n                \n              \n            \n            \n              (\n              \n                σ\n                \n                  (\n                  t\n                  )\n                \n              \n              \n                )\n                \n                  2\n                \n              \n              +\n              ϵ\n            \n          \n        \n        ,\n        \n        \n          y\n          \n            i\n          \n          \n            (\n            t\n            )\n          \n        \n        =\n        \n          γ\n          \n            i\n          \n        \n        \n          \n            \n              \n                \n                  x\n                  \n                    i\n                  \n                \n                ^\n              \n            \n          \n          \n            (\n            t\n            )\n          \n        \n        +\n        \n          β\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\hat {x_{i}}}^{(t)}={\\frac {x_{i}^{(t)}-\\mu ^{(t)}}{\\sqrt {(\\sigma ^{(t)})^{2}+\\epsilon }}},\\quad y_{i}^{(t)}=\\gamma _{i}{\\hat {x_{i}}}^{(t)}+\\beta _{i}}\n  \n\nwhere:\n\n  \n    \n      \n        \n          μ\n          \n            (\n            t\n            )\n          \n        \n        =\n        \n          \n            1\n            D\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            D\n          \n        \n        \n          x\n          \n            i\n          \n          \n            (\n            t\n            )\n          \n        \n        ,\n        \n        (\n        \n          σ\n          \n            (\n            t\n            )\n          \n        \n        \n          )\n          \n            2\n          \n        \n        =\n        \n          \n            1\n            D\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            D\n          \n        \n        (\n        \n          x\n          \n            i\n          \n          \n            (\n            t\n            )\n          \n        \n        −\n        \n          μ\n          \n            (\n            t\n            )\n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mu ^{(t)}={\\frac {1}{D}}\\sum _{i=1}^{D}x_{i}^{(t)},\\quad (\\sigma ^{(t)})^{2}={\\frac {1}{D}}\\sum _{i=1}^{D}(x_{i}^{(t)}-\\mu ^{(t)})^{2}}\n\nRoot mean square layer normalization\nRoot mean square layer normalization (RMSNorm) changes LayerNorm by:\n\n  \n    \n      \n        \n          \n            \n              \n                x\n                \n                  i\n                \n              \n              ^\n            \n          \n        \n        =\n        \n          \n            \n              x\n              \n                i\n              \n            \n            \n              \n                \n                  1\n                  D\n                \n              \n              \n                ∑\n                \n                  i\n                  =\n                  1\n                \n                \n                  D\n                \n              \n              \n                x\n                \n                  i\n                \n                \n                  2\n                \n              \n            \n          \n        \n        ,\n        \n        \n          y\n          \n            i\n          \n        \n        =\n        γ\n        \n          \n            \n              \n                x\n                \n                  i\n                \n              \n              ^\n            \n          \n        \n        +\n        β\n      \n    \n    {\\displaystyle {\\hat {x_{i}}}={\\frac {x_{i}}{\\sqrt {{\\frac {1}{D}}\\sum _{i=1}^{D}x_{i}^{2}}}},\\quad y_{i}=\\gamma {\\hat {x_{i}}}+\\beta }\n  \n\nEssentially, it is LayerNorm where we enforce \n  \n    \n      \n        μ\n        ,\n        ϵ\n        =\n        0\n      \n    \n    {\\displaystyle \\mu ,\\epsilon =0}\n  \n.\n\nAdaptive\nAdaptive layer norm (adaLN) computes the \n  \n    \n      \n        γ\n        ,\n        β\n      \n    \n    {\\displaystyle \\gamma ,\\beta }\n  \n in a LayerNorm not from the layer activation itself, but from other data. It was first proposed for CNNs, and has been used effectively in diffusion transformers (DiTs). For example, in a DiT, the conditioning information (such as a text encoding vector) is processed by a multilayer perceptron into \n  \n    \n      \n        γ\n        ,\n        β\n      \n    \n    {\\displaystyle \\gamma ,\\beta }\n  \n, which is then applied in the LayerNorm module of a transformer.\n\nWeight normalization\nWeight normalization (WeightNorm) is a technique inspired by BatchNorm that normalizes weight matrices in a neural network, rather than its activations.\nOne example is spectral normalization, which divides weight matrices by their spectral norm. The spectral normalization is used in generative adversarial networks (GANs) such as the Wasserstein GAN. The spectral radius can be efficiently computed by the following algorithm:\n\nINPUT matrix \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n and initial guess \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n\nIterate \n  \n    \n      \n        x\n        ↦\n        \n          \n            1\n            \n              ‖\n              W\n              x\n              \n                ‖\n                \n                  2\n                \n              \n            \n          \n        \n        W\n        x\n      \n    \n    {\\displaystyle x\\mapsto {\\frac {1}{\\|Wx\\|_{2}}}Wx}\n  \n to convergence \n  \n    \n      \n        \n          x\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle x^{*}}\n  \n. This is the eigenvector of \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n with eigenvalue \n  \n    \n      \n        ‖\n        W\n        \n          ‖\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle \\|W\\|_{s}}\n  \n.\n\nRETURN \n  \n    \n      \n        \n          x\n          \n            ∗\n          \n        \n        ,\n        ‖\n        W\n        \n          x\n          \n            ∗\n          \n        \n        \n          ‖\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x^{*},\\|Wx^{*}\\|_{2}}\n  \n\nBy reassigning \n  \n    \n      \n        \n          W\n          \n            i\n          \n        \n        ←\n        \n          \n            \n              W\n              \n                i\n              \n            \n            \n              ‖\n              \n                W\n                \n                  i\n                \n              \n              \n                ‖\n                \n                  s\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle W_{i}\\leftarrow {\\frac {W_{i}}{\\|W_{i}\\|_{s}}}}\n  \n after each update of the discriminator, we can upper-bound \n  \n    \n      \n        ‖\n        \n          W\n          \n            i\n          \n        \n        \n          ‖\n          \n            s\n          \n        \n        ≤\n        1\n      \n    \n    {\\displaystyle \\|W_{i}\\|_{s}\\leq 1}\n  \n, and thus upper-bound \n  \n    \n      \n        ‖\n        D\n        \n          ‖\n          \n            L\n          \n        \n      \n    \n    {\\displaystyle \\|D\\|_{L}}\n  \n.\nThe algorithm can be further accelerated by memoization: at step \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, store \n  \n    \n      \n        \n          x\n          \n            i\n          \n          \n            ∗\n          \n        \n        (\n        t\n        )\n      \n    \n    {\\displaystyle x_{i}^{*}(t)}\n  \n. Then, at step \n  \n    \n      \n        t\n        +\n        1\n      \n    \n    {\\displaystyle t+1}\n  \n, use \n  \n    \n      \n        \n          x\n          \n            i\n          \n          \n            ∗\n          \n        \n        (\n        t\n        )\n      \n    \n    {\\displaystyle x_{i}^{*}(t)}\n  \n as the initial guess for the algorithm. Since \n  \n    \n      \n        \n          W\n          \n            i\n          \n        \n        (\n        t\n        +\n        1\n        )\n      \n    \n    {\\displaystyle W_{i}(t+1)}\n  \n is very close to \n  \n    \n      \n        \n          W\n          \n            i\n          \n        \n        (\n        t\n        )\n      \n    \n    {\\displaystyle W_{i}(t)}\n  \n, so is \n  \n    \n      \n        \n          x\n          \n            i\n          \n          \n            ∗\n          \n        \n        (\n        t\n        )\n      \n    \n    {\\displaystyle x_{i}^{*}(t)}\n  \n to \n  \n    \n      \n        \n          x\n          \n            i\n          \n          \n            ∗\n          \n        \n        (\n        t\n        +\n        1\n        )\n      \n    \n    {\\displaystyle x_{i}^{*}(t+1)}\n  \n, thus allowing rapid convergence.\n\nCNN-specific normalization\nThere are some activation normalization techniques that are only used for CNNs.\n\nResponse normalization\nLocal response normalization was used in AlexNet. It was applied in a convolutional layer, just after a nonlinear activation function. It was defined by:\n\n  \n    \n      \n        \n          b\n          \n            x\n            ,\n            y\n          \n          \n            i\n          \n        \n        =\n        \n          \n            \n              a\n              \n                x\n                ,\n                y\n              \n              \n                i\n              \n            \n            \n              \n                (\n                \n                  k\n                  +\n                  α\n                  \n                    ∑\n                    \n                      j\n                      =\n                      max\n                      (\n                      0\n                      ,\n                      i\n                      −\n                      n\n                      \n                        /\n                      \n                      2\n                      )\n                    \n                    \n                      min\n                      (\n                      N\n                      −\n                      1\n                      ,\n                      i\n                      +\n                      n\n                      \n                        /\n                      \n                      2\n                      )\n                    \n                  \n                  \n                    \n                      (\n                      \n                        a\n                        \n                          x\n                          ,\n                          y\n                        \n                        \n                          j\n                        \n                      \n                      )\n                    \n                    \n                      2\n                    \n                  \n                \n                )\n              \n              \n                β\n              \n            \n          \n        \n      \n    \n    {\\displaystyle b_{x,y}^{i}={\\frac {a_{x,y}^{i}}{\\left(k+\\alpha \\sum _{j=\\max(0,i-n/2)}^{\\min(N-1,i+n/2)}\\left(a_{x,y}^{j}\\right)^{2}\\right)^{\\beta }}}}\n  \n\nwhere \n  \n    \n      \n        \n          a\n          \n            x\n            ,\n            y\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle a_{x,y}^{i}}\n  \n is the activation of the neuron at location \n  \n    \n      \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle (x,y)}\n  \n and channel \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n. I.e., each pixel in a channel is suppressed by the activations of the same pixel in its adjacent channels.\n\n  \n    \n      \n        k\n        ,\n        n\n        ,\n        α\n        ,\n        β\n      \n    \n    {\\displaystyle k,n,\\alpha ,\\beta }\n  \n are hyperparameters picked by using a validation set.\nIt was a variant of the earlier local contrast normalization.\n\n  \n    \n      \n        \n          b\n          \n            x\n            ,\n            y\n          \n          \n            i\n          \n        \n        =\n        \n          \n            \n              a\n              \n                x\n                ,\n                y\n              \n              \n                i\n              \n            \n            \n              \n                (\n                \n                  k\n                  +\n                  α\n                  \n                    ∑\n                    \n                      j\n                      =\n                      max\n                      (\n                      0\n                      ,\n                      i\n                      −\n                      n\n                      \n                        /\n                      \n                      2\n                      )\n                    \n                    \n                      min\n                      (\n                      N\n                      −\n                      1\n                      ,\n                      i\n                      +\n                      n\n                      \n                        /\n                      \n                      2\n                      )\n                    \n                  \n                  \n                    \n                      (\n                      \n                        \n                          a\n                          \n                            x\n                            ,\n                            y\n                          \n                          \n                            j\n                          \n                        \n                        −\n                        \n                          \n                            \n                              \n                                a\n                                ¯\n                              \n                            \n                          \n                          \n                            x\n                            ,\n                            y\n                          \n                          \n                            j\n                          \n                        \n                      \n                      )\n                    \n                    \n                      2\n                    \n                  \n                \n                )\n              \n              \n                β\n              \n            \n          \n        \n      \n    \n    {\\displaystyle b_{x,y}^{i}={\\frac {a_{x,y}^{i}}{\\left(k+\\alpha \\sum _{j=\\max(0,i-n/2)}^{\\min(N-1,i+n/2)}\\left(a_{x,y}^{j}-{\\bar {a}}_{x,y}^{j}\\right)^{2}\\right)^{\\beta }}}}\n  \n\nwhere \n  \n    \n      \n        \n          \n            \n              \n                a\n                ¯\n              \n            \n          \n          \n            x\n            ,\n            y\n          \n          \n            j\n          \n        \n      \n    \n    {\\displaystyle {\\bar {a}}_{x,y}^{j}}\n  \n is the average activation in a small window centered on location \n  \n    \n      \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle (x,y)}\n  \n and channel \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n. The hyperparameters \n  \n    \n      \n        k\n        ,\n        n\n        ,\n        α\n        ,\n        β\n      \n    \n    {\\displaystyle k,n,\\alpha ,\\beta }\n  \n, and the size of the small window, are picked by using a validation set.\nSimilar methods were called divisive normalization, as they divide activations by a number depending on the activations. They were originally inspired by biology, where it was used to explain nonlinear responses of cortical neurons and nonlinear masking in visual perception.\nBoth kinds of local normalization were obviated by batch normalization, which is a more global form of normalization.\nResponse normalization reappeared in ConvNeXT-2 as global response normalization.\n\nGroup normalization\nGroup normalization (GroupNorm) is a technique also solely used for CNNs. It can be understood as the LayerNorm for CNN applied once per channel group.\nSuppose at a layer \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  \n, there are channels \n  \n    \n      \n        1\n        ,\n        2\n        ,\n        …\n        ,\n        C\n      \n    \n    {\\displaystyle 1,2,\\dots ,C}\n  \n, then it is partitioned into groups \n  \n    \n      \n        \n          g\n          \n            1\n          \n        \n        ,\n        \n          g\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          g\n          \n            G\n          \n        \n      \n    \n    {\\displaystyle g_{1},g_{2},\\dots ,g_{G}}\n  \n. Then, LayerNorm is applied to each group.\n\nInstance normalization\nInstance normalization (InstanceNorm), or contrast normalization, is a technique first developed for neural style transfer, and is also only used for CNNs. It can be understood as the LayerNorm for CNN applied once per channel, or equivalently, as group normalization where each group consists of a single channel:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  μ\n                  \n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    \n                      H\n                      W\n                    \n                  \n                \n                \n                  ∑\n                  \n                    h\n                    =\n                    1\n                  \n                  \n                    H\n                  \n                \n                \n                  ∑\n                  \n                    w\n                    =\n                    1\n                  \n                  \n                    W\n                  \n                \n                \n                  x\n                  \n                    h\n                    ,\n                    w\n                    ,\n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n              \n            \n            \n              \n                (\n                \n                  σ\n                  \n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    \n                      H\n                      W\n                    \n                  \n                \n                \n                  ∑\n                  \n                    h\n                    =\n                    1\n                  \n                  \n                    H\n                  \n                \n                \n                  ∑\n                  \n                    w\n                    =\n                    1\n                  \n                  \n                    W\n                  \n                \n                (\n                \n                  x\n                  \n                    h\n                    ,\n                    w\n                    ,\n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n                −\n                \n                  μ\n                  \n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n                \n                  )\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                        x\n                        ^\n                      \n                    \n                  \n                  \n                    h\n                    ,\n                    w\n                    ,\n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    \n                      \n                        \n                          \n                            \n                              x\n                              ^\n                            \n                          \n                        \n                        \n                          h\n                          ,\n                          w\n                          ,\n                          c\n                        \n                        \n                          (\n                          l\n                          )\n                        \n                      \n                      −\n                      \n                        μ\n                        \n                          c\n                        \n                        \n                          (\n                          l\n                          )\n                        \n                      \n                    \n                    \n                      (\n                      \n                        σ\n                        \n                          c\n                        \n                        \n                          (\n                          l\n                          )\n                        \n                      \n                      \n                        )\n                        \n                          2\n                        \n                      \n                      +\n                      ϵ\n                    \n                  \n                \n              \n            \n            \n              \n                \n                  y\n                  \n                    h\n                    ,\n                    w\n                    ,\n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n              \n              \n                \n                =\n                \n                  γ\n                  \n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n                \n                  \n                    \n                      \n                        x\n                        ^\n                      \n                    \n                  \n                  \n                    h\n                    ,\n                    w\n                    ,\n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n                +\n                \n                  β\n                  \n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\mu _{c}^{(l)}&={\\frac {1}{HW}}\\sum _{h=1}^{H}\\sum _{w=1}^{W}x_{h,w,c}^{(l)}\\\\(\\sigma _{c}^{(l)})^{2}&={\\frac {1}{HW}}\\sum _{h=1}^{H}\\sum _{w=1}^{W}(x_{h,w,c}^{(l)}-\\mu _{c}^{(l)})^{2}\\\\{\\hat {x}}_{h,w,c}^{(l)}&={\\frac {{\\hat {x}}_{h,w,c}^{(l)}-\\mu _{c}^{(l)}}{\\sqrt {(\\sigma _{c}^{(l)})^{2}+\\epsilon }}}\\\\y_{h,w,c}^{(l)}&=\\gamma _{c}^{(l)}{\\hat {x}}_{h,w,c}^{(l)}+\\beta _{c}^{(l)}\\end{aligned}}}\n\nAdaptive instance normalization\nAdaptive instance normalization (AdaIN) is a variant of instance normalization, designed specifically for neural style transfer with CNNs, rather than just CNNs in general.\nIn the AdaIN method of style transfer, we take a CNN and two input images, one for content and one for style. Each image is processed through the same CNN, and at a certain layer \n  \n    \n      \n        l\n      \n    \n    {\\displaystyle l}\n  \n, AdaIn is applied.\nLet \n  \n    \n      \n        \n          x\n          \n            (\n            l\n            )\n            ,\n            \n               content\n            \n          \n        \n      \n    \n    {\\displaystyle x^{(l),{\\text{ content}}}}\n  \n be the activation in the content image, and \n  \n    \n      \n        \n          x\n          \n            (\n            l\n            )\n            ,\n            \n               style\n            \n          \n        \n      \n    \n    {\\displaystyle x^{(l),{\\text{ style}}}}\n  \n be the activation in the style image. Then, AdaIn first computes the mean and variance of the activations of the content image \n  \n    \n      \n        \n          x\n          \n            ′\n            \n              (\n              l\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle x'^{(l)}}\n  \n, then uses those as the \n  \n    \n      \n        γ\n        ,\n        β\n      \n    \n    {\\displaystyle \\gamma ,\\beta }\n  \n for InstanceNorm on \n  \n    \n      \n        \n          x\n          \n            (\n            l\n            )\n            ,\n            \n               content\n            \n          \n        \n      \n    \n    {\\displaystyle x^{(l),{\\text{ content}}}}\n  \n. Note that \n  \n    \n      \n        \n          x\n          \n            (\n            l\n            )\n            ,\n            \n               style\n            \n          \n        \n      \n    \n    {\\displaystyle x^{(l),{\\text{ style}}}}\n  \n itself remains unchanged. Explicitly, we have:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  y\n                  \n                    h\n                    ,\n                    w\n                    ,\n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                    ,\n                    \n                       content\n                    \n                  \n                \n              \n              \n                \n                =\n                \n                  σ\n                  \n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                    ,\n                    \n                       style\n                    \n                  \n                \n                \n                  (\n                  \n                    \n                      \n                        \n                          x\n                          \n                            h\n                            ,\n                            w\n                            ,\n                            c\n                          \n                          \n                            (\n                            l\n                            )\n                            ,\n                            \n                               content\n                            \n                          \n                        \n                        −\n                        \n                          μ\n                          \n                            c\n                          \n                          \n                            (\n                            l\n                            )\n                            ,\n                            \n                               content\n                            \n                          \n                        \n                      \n                      \n                        (\n                        \n                          σ\n                          \n                            c\n                          \n                          \n                            (\n                            l\n                            )\n                            ,\n                            \n                               content\n                            \n                          \n                        \n                        \n                          )\n                          \n                            2\n                          \n                        \n                        +\n                        ϵ\n                      \n                    \n                  \n                  )\n                \n                +\n                \n                  μ\n                  \n                    c\n                  \n                  \n                    (\n                    l\n                    )\n                    ,\n                    \n                       style\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}y_{h,w,c}^{(l),{\\text{ content}}}&=\\sigma _{c}^{(l),{\\text{ style}}}\\left({\\frac {x_{h,w,c}^{(l),{\\text{ content}}}-\\mu _{c}^{(l),{\\text{ content}}}}{\\sqrt {(\\sigma _{c}^{(l),{\\text{ content}}})^{2}+\\epsilon }}}\\right)+\\mu _{c}^{(l),{\\text{ style}}}\\end{aligned}}}\n\nTransformers\nSome normalization methods were designed for use in transformers.\nThe original 2017 transformer used the \"post-LN\" configuration for its LayerNorms. It was difficult to train, and required careful hyperparameter tuning and a \"warm-up\" in learning rate, where it starts small and gradually increases. The pre-LN convention, proposed several times in 2018, was found to be easier to train, requiring no warm-up, leading to faster convergence.\nFixNorm and ScaleNorm both normalize activation vectors in a transformer. The FixNorm method divides the output vectors from a transformer by their L2 norms, then multiplies by a learned parameter \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n  \n. The ScaleNorm replaces all LayerNorms inside a transformer by division with L2 norm, then multiplying by a learned parameter \n  \n    \n      \n        \n          g\n          ′\n        \n      \n    \n    {\\displaystyle g'}\n  \n (shared by all ScaleNorm modules of a transformer). Query-Key normalization (QKNorm) normalizes query and key vectors to have unit L2 norm.\nIn nGPT, many vectors are normalized to have unit L2 norm: hidden state vectors, input and output embedding vectors, weight matrix columns, and query and key vectors.\n\nMiscellaneous\nGradient normalization (GradNorm) normalizes gradient vectors during backpropagation.\n\nSee also\nData preprocessing\nFeature scaling\n\nReferences\nFurther reading\n\"Normalization Layers\". labml.ai Deep Learning Paper Implementations. Retrieved 2024-08-07.\n",
  "categories": [
    "Category:Articles with example Python (programming language) code",
    "Category:Articles with short description",
    "Category:CS1 errors: missing periodical",
    "Category:Deep learning",
    "Category:Machine learning",
    "Category:Neural networks",
    "Category:Short description matches Wikidata",
    "Category:Statistical data transformation"
  ],
  "archived_date": "20241220_214805",
  "source_url": "https://en.wikipedia.org/wiki/Normalization_(machine_learning)"
}